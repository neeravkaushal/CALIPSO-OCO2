{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**For 2018-05-01, there are 7600 sif footprints scattered over 46 tiles.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PAR Processing Started...\n",
      "\n",
      "h10v05 h10v06 h10v08 h10v09 h10v10 h11v03 h11v04 h11v11 h12v02 h12v03 h12v12 h12v13 h13v01 h13v02 h13v13 h14v01 h17v00 h18v00 h18v01 h18v03 h18v04 h19v01 h19v02 h19v06 h19v07 h20v03 h20v08 h20v09 h20v10 h20v11 h20v12 h21v02 h21v04 h22v04 h23v03 h23v06 h24v04 h25v04 h27v12 h28v06 h28v11 h29v07 h29v08 h29v10 h29v11 h30v09 \n",
      "\n",
      "REF Processing Started...\n",
      "\n",
      "h10v05 h10v06 h10v08 h10v09 h10v10 h11v03 h11v04 h11v11 h12v02 h12v03 h12v12 h12v13 h13v01 h13v02 h13v13 h14v01 h17v00 h18v00 h18v01 h18v03 h18v04 h19v01 h19v02 h19v06 h19v07 h20v03 h20v08 h20v09 h20v10 h20v11 h20v12 h21v02 h21v04 h22v04 h23v03 h23v06 h24v04 h25v04 h27v12 h28v06 h28v11 h29v07 h29v08 h29v10 h29v11 h30v09 \n",
      "\n",
      "FPAR Processing Started...\n",
      "\n",
      "h10v05 h10v06 h10v08 h10v09 h10v10 h11v03 h11v04 h11v11 h12v02 h12v03 h12v12 h12v13 h13v01 h13v02 h13v13 h14v01 h17v00 h18v00 h18v01 h18v03 h18v04 h19v01 h19v02 h19v06 h19v07 h20v03 h20v08 h20v09 h20v10 h20v11 h20v12 h21v02 h21v04 h22v04 h23v03 h23v06 h24v04 h25v04 h27v12 h28v06 h28v11 h29v07 h29v08 h29v10 h29v11 h30v09 ------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**For 2018-05-02, there are 9236 sif footprints scattered over 49 tiles.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PAR Processing Started...\n",
      "\n",
      "h07v05 h08v04 h09v03 h11v02 h11v07 h11v08 h11v09 h12v03 h12v04 h12v09 h12v10 h12v11 h13v02 h13v03 h14v01 h16v02 h16v06 h16v07 h17v00 h18v00 h18v01 h18v02 h18v03 h19v01 h19v03 h19v04 h20v02 h20v05 h20v06 h21v02 h21v03 h21v07 h21v08 h21v09 h21v10 h22v02 h22v04 h23v03 h24v03 h24v06 h25v04 h25v06 h25v07 h26v04 h28v12 h29v12 h30v11 h31v09 h31v10 \n",
      "\n",
      "REF Processing Started...\n",
      "\n",
      "h07v05 h08v04 h09v03 h11v02 h11v07 h11v08 h11v09 h12v03 h12v04 h12v09 h12v10 h12v11 h13v02 h13v03 h14v01 h16v02 h16v06 h16v07 h17v00 h18v00 h18v01 h18v02 h18v03 h19v01 h19v03 h19v04 h20v02 h20v05 h20v06 h21v02 h21v03 h21v07 h21v08 h21v09 h21v10 h22v02 h22v04 h23v03 h24v03 h24v06 h25v04 h25v06 h25v07 h26v04 h28v12 h29v12 h30v11 h31v09 h31v10 \n",
      "\n",
      "FPAR Processing Started...\n",
      "\n",
      "h07v05 h08v04 h09v03 h11v02 h11v07 h11v08 h11v09 h12v03 h12v04 h12v09 h12v10 h12v11 h13v02 h13v03 h14v01 h16v02 h16v06 h16v07 h17v00 h18v00 h18v01 h18v02 h18v03 h19v01 h19v03 h19v04 h20v02 h20v05 h20v06 h21v02 h21v03 h21v07 h21v08 h21v09 h21v10 h22v02 h22v04 h23v03 h24v03 h24v06 h25v04 h25v06 h25v07 h26v04 h28v12 h29v12 h30v11 h31v09 h31v10 ------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**For 2018-05-03, there are 10039 sif footprints scattered over 52 tiles.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "PAR Processing Started...\n",
      "\n",
      "h07v07 h08v06 h08v07 h09v04 h10v03 h11v02 h12v02 h13v03 h13v04 h13v09 h13v10 h13v11 h14v01 h14v02 h14v03 h15v01 h16v00 h16v01 h16v02 h17v00 h17v01 h17v04 h17v05 h17v06 h17v07 h17v08 h18v00 h18v01 h18v02 h19v01 h19v02 h19v03 h20v01 h20v02 h20v04 h21v06 h22v02 h22v03 h22v06 h22v07 h22v08 h22v11 h23v04 h24v03 h24v05 h26v04 h26v06 h27v07 h27v08 h27v09 h31v07 h32v09 \n",
      "\n",
      "REF Processing Started...\n",
      "\n",
      "h07v07 h08v06 h08v07 h09v04 h10v03 h11v02 h12v02 h13v03 h13v04 h13v09 h13v10 h13v11 h14v01 h14v02 h14v03 h15v01 h16v00 h16v01 h16v02 h17v00 h17v01 h17v04 h17v05 h17v06 h17v07 h17v08 h18v00 h18v01 h18v02 h19v01 h19v02 h19v03 h20v01 h20v02 h20v04 h21v06 h22v02 h22v03 h22v06 h22v07 h22v08 h22v11 h23v04 h24v03 h24v05 h26v04 h26v06 h27v07 h27v08 h27v09 h31v07 h32v09 \n",
      "\n",
      "FPAR Processing Started...\n",
      "\n",
      "h07v07 h08v06 h08v07 h09v04 h10v03 h11v02 h12v02 h13v03 h13v04 h13v09 h13v10 h13v11 h14v01 h14v02 h14v03 h15v01 h16v00 h16v01 h16v02 h17v00 h17v01 h17v04 h17v05 h17v06 h17v07 h17v08 h18v00 h18v01 h18v02 h19v01 h19v02 h19v03 h20v01 h20v02 h20v04 h21v06 h22v02 h22v03 h22v06 h22v07 h22v08 h22v11 h23v04 h24v03 h24v05 h26v04 h26v06 h27v07 h27v08 h27v09 h31v07 h32v09 "
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell  #---- Output all jupyter lab inputs instead of the last one\n",
    "from IPython.display import Markdown, display\n",
    "InteractiveShell.ast_node_interactivity = \"all\"             \n",
    "import glob #---------------------------------------------------- To read the files or folders in a system directory\n",
    "from netCDF4 import Dataset #------------------------------------ To read nc , nc4 and hdf4 files\n",
    "import numpy as np\n",
    "import datetime\n",
    "import h5py #---------------------------------------------------- To read hdf5 files\n",
    "from scipy import spatial #-------------------------------------- To extract the values and indices of k nearest neighbors\n",
    "import pandas as pd\n",
    "from ast import literal_eval #----------------------------------- For literal evaluation of a string to extract python objects\n",
    "from pyproj import Proj, transform #----------------------------- To interconvert different projections\n",
    "import warnings #------------------------------------------------ To suppress warnings\n",
    "from photutils.utils import ShepardIDWInterpolator as idw #------ To use Shepard's Inverse Distance Weighing Interpolation tool\n",
    "import re #------------------------------------------------------ To replace characters in a string\n",
    "warnings.simplefilter('ignore')\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "### Define Non-Iterative Functions and Variables\n",
    "\n",
    "in_proj  = Proj('+proj=sinu +R=6371007.181 +nadgrids=@null +wktext') #------ Specify input projection\n",
    "out_proj = Proj(init='epsg:4326') #----------------------------------------- Specify output projection\n",
    "\n",
    "\n",
    "def Times(x): #------------------------------------------Extract Time from sounding ID. NOTE the time format is HH:MM:SSSS\n",
    "    y   = str(x)\n",
    "    yy  = y[8:]\n",
    "    yyy = '{}:{}:{}'.format(yy[:2], yy[2:4], yy[4:])\n",
    "    return yyy\n",
    "\n",
    "hours = [0, 3000000, 6000000, 9000000, 12000000, 15000000, 18000000, 21000000, 23595900]\n",
    "def f(x): #---------------------------------------------Extract the hour interval of sif time (which  is in seconds)\n",
    "    for i in range(len(hours)):\n",
    "        if (x>hours[i]) and (x<hours[i+1]):\n",
    "            lb = hours[i]\n",
    "            ub = hours[i+1]\n",
    "            return lb,ub\n",
    "            break\n",
    "\n",
    "def format_time(t): #----------------------------------- Format the time into HH:MM:SSSS for the raw format HH:MM:SSSSSSSS\n",
    "    s = t\n",
    "    return s[:-4]\n",
    "\n",
    "def nn(latitude_list,longitude_list,target): #---------- Find the index of nearest neighbor (NOTE: absolute difference)\n",
    "    target_lat, target_lon = target[1], target[0]\n",
    "    d = [abs(latitude-target_lat) + abs(longitude-target_lon) for latitude,longitude in zip(latitude_list,longitude_list)]\n",
    "    return np.argmin(d)\n",
    "\n",
    "data = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3)\n",
    "def tile_finder(Lat,Lon): #----------------------------- Find modis tile numbers in which the argument lat,lon lies\n",
    "    in_tile = False\n",
    "    i = 0\n",
    "    while(not in_tile):\n",
    "        in_tile = Lat >= data[i, 4] and Lat <= data[i, 5] and Lon >= data[i, 2] and Lon <= data[i, 3]\n",
    "        i += 1\n",
    "    V = str(int(data[i-1, 0])).zfill(2)\n",
    "    H = str(int(data[i-1, 1])).zfill(2)\n",
    "    return H,V\n",
    "\n",
    "def extract_pixel_coordinates(ULx,Uly,LRx,LRy,shape):\n",
    "    x        = np.linspace(ULx, LRx, shape[0], endpoint=False) + abs((ULx-LRx)/(2*shape[0]))\n",
    "    y        = np.linspace(ULy, LRy, shape[0], endpoint=False) - abs((ULy-LRy)/(2*shape[0]))\n",
    "    xx, yy   = np.meshgrid(x,y)\n",
    "    xs       = xx.flatten()\n",
    "    ys       = yy.flatten()\n",
    "    plon, plat = transform(in_proj, out_proj, xs, ys)\n",
    "    return plon, plat\n",
    "\n",
    "def temporal_interpolation(time1,val1,time2,val2,timeX):\n",
    "    df    = pd.DataFrame( [(time1, val1) , (time2, val2)] , columns=['Times','Values'] ) \n",
    "    df    = df.set_index('Times')\n",
    "    df    = pd.Series(df['Values'], index=df.index)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    inter = df.resample('S').interpolate(method='linear')\n",
    "    valX  = inter.loc[timeX]\n",
    "    return valX\n",
    "\n",
    "sif_file_list     = glob.glob('OCO2_sif/*.nc4')    #--------------------------------------------- List of all OCO2 files\n",
    "calipso_file_list = glob.glob('OCO2_calipso/*.h5') #--------------------------------------------- List of all OCO2-CALIPSO files\n",
    "fpar_file_list    = glob.glob('MCD15A3H/*.hdf')    \n",
    "par_folder_list   = glob.glob('MCD18A2/*')\n",
    "ref_folder_list   = glob.glob('MCD43A4/*')\n",
    "\n",
    "data              = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3) #------ File having tile numbers and IDs\n",
    "\n",
    "for sif_file in sif_file_list: #---------------------------------------------------------------------------Read one sif file at a time\n",
    "    sif_date        = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%Y-%m-%d\") #-----Extract sif date\n",
    "    sif_julian_day  = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%j\") #-----------Extract sif julian day\n",
    "    sif             = Dataset(sif_file, mode='r') #-----------------------------------------------------------Open sif file\n",
    "    calipso_df_list = [] #------------------------------------------------------------------------------------Create an empty list\n",
    "    for calipso_file in calipso_file_list: #------------------------------------------------------------------Loop through all calipso files\n",
    "        calipso_date      = datetime.datetime.strptime(calipso_file.split('_')[5], '%y%m%d').strftime(\"%Y-%m-%d\")\n",
    "        if calipso_date  == sif_date: #----------------------------------------------------------------------------If calipso date matches sif date,\n",
    "            calipso       = h5py.File(calipso_file, mode='r') #----------------------------------------------------open the calipso file\n",
    "            calipso_ID    = calipso['OCO2_sounding_id'                                           ][:]\n",
    "            calipso_dist  = calipso['matchup_distance_km'                                        ][:]\n",
    "            calipso_index = calipso['matchup_Xindex'                                             ][:]\n",
    "            calipso_dfs   = pd.DataFrame({'sounding_id':calipso_ID.flatten(),'Xindex':calipso_index.flatten(),'Xdistance':calipso_dist.flatten()}) #-----Create dataframe with variables\n",
    "            calipso_dfs[calipso_dfs.Xindex==-999.0] = np.nan #------- Replace missing values with nan\n",
    "            calipso_dfs.dropna(inplace=True) #----------------------- Drop missing values of Xindex\n",
    "            calipso_dfs[calipso_dfs.Xdistance>=2.0] = np.nan\n",
    "            calipso_dfs.dropna(inplace=True)\n",
    "            calipso_df_list.append(calipso_dfs) #-------------------- Add all calipso dataframes into a list\n",
    "    calipso_df                     = pd.concat(calipso_df_list, ignore_index = True).drop_duplicates() #---------Create a final calipso dataframe for a day\n",
    "\n",
    "    cloud_albedo                   = sif.groups['Cloud'].variables['albedo'                 ][:].flatten() #--------Read sif variables and flatten them\n",
    "    cloud_flag                     = sif.groups['Cloud'].variables['cloud_flag'             ][:].flatten()\n",
    "    cloud_co2_ratio                = sif.groups['Cloud'].variables['co2_ratio'              ][:].flatten()\n",
    "    cloud_delta_surface_pressure   = sif.groups['Cloud'].variables['delta_surface_pressure' ][:].flatten()\n",
    "    cloud_o2_ratio                 = sif.groups['Cloud'].variables['o2_ratio'               ][:].flatten()\n",
    "    vapor_pressure_deficit         = sif.groups['Meteo'].variables['vapor_pressure_deficit' ][:].flatten()\n",
    "    temperature_2m                 = sif.groups['Meteo'].variables['2m_temperature'         ][:].flatten()\n",
    "    temperature_skin               = sif.groups['Meteo'].variables['skin_temperature'       ][:].flatten()\n",
    "    specific_humidity              = sif.groups['Meteo'].variables['specific_humidity'      ][:].flatten()\n",
    "    surface_pressure               = sif.groups['Meteo'].variables['surface_pressure'       ][:].flatten()\n",
    "    wind_speed                     = sif.groups['Meteo'].variables['wind_speed'             ][:].flatten()\n",
    "    continuum_radiance_757nm       = sif.variables         ['continuum_radiance_757nm'      ][:].flatten()\n",
    "    continuum_radiance_771nm       = sif.variables         ['continuum_radiance_771nm'      ][:].flatten()\n",
    "    daily_correction_factor        = sif.variables         ['daily_correction_factor'       ][:].flatten()\n",
    "    footprint                      = sif.variables         ['footprint'                     ][:].flatten()\n",
    "    IGBP_index                     = sif.variables         ['IGBP_index'                    ][:].flatten()\n",
    "    latitude                       = sif.variables         ['latitude'                      ][:].flatten()\n",
    "    longitude                      = sif.variables         ['longitude'                     ][:].flatten()\n",
    "    measurement_mode               = sif.variables         ['measurement_mode'              ][:].flatten()\n",
    "    orbit_number                   = sif.variables         ['orbit_number'                  ][:].flatten()\n",
    "    reduced_chi2_757nm             = sif.variables         ['reduced_chi2_757nm'            ][:].flatten()\n",
    "    reduced_chi2_771nm             = sif.variables         ['reduced_chi2_771nm'            ][:].flatten()\n",
    "    sensor_azimuth_angle           = sif.variables         ['sensor_azimuth_angle'          ][:].flatten()\n",
    "    sensor_zenith_angle            = sif.variables         ['sensor_zenith_angle'           ][:].flatten()\n",
    "    rSIF_757nm                     = sif.variables         ['SIF_757nm'                     ][:].flatten() #----------Use this condition for next block\n",
    "    SIF_757nm_relative             = sif.variables         ['SIF_757nm_relative'            ][:].flatten()\n",
    "    SIF_757nm_uncert               = sif.variables         ['SIF_757nm_uncert'              ][:].flatten()\n",
    "    SIF_771nm                      = sif.variables         ['SIF_771nm'                     ][:].flatten()\n",
    "    SIF_771nm_relative             = sif.variables         ['SIF_771nm_relative'            ][:].flatten()\n",
    "    SIF_771nm_uncert               = sif.variables         ['SIF_771nm_uncert'              ][:].flatten()\n",
    "    solar_azimuth_angle            = sif.variables         ['solar_azimuth_angle'           ][:].flatten()\n",
    "    solar_zenith_angle             = sif.variables         ['solar_zenith_angle'            ][:].flatten()\n",
    "    sounding_id                    = sif.variables         ['sounding_id'                   ][:].flatten()\n",
    "    surface_altitude               = sif.variables         ['surface_altitude'              ][:].flatten()\n",
    "    time                           = sif.variables         ['time'                          ][:].flatten()\n",
    "    uncorrected_SIF_757nm          = sif.variables         ['uncorrected_SIF_757nm'         ][:].flatten()\n",
    "    uncorrected_SIF_757nm_relative = sif.variables         ['uncorrected_SIF_757nm_relative'][:].flatten()\n",
    "    uncorrected_SIF_771nm          = sif.variables         ['uncorrected_SIF_771nm'         ][:].flatten()\n",
    "    uncorrected_SIF_771nm_relative = sif.variables         ['uncorrected_SIF_771nm_relative'][:].flatten()\n",
    "    \n",
    "    \n",
    "    SIF_757nm                      = rSIF_757nm                     [rSIF_757nm>0]  #--------- Select only those values for which SIF_757nm > 0\n",
    "    cloud_albedo                   = cloud_albedo                   [rSIF_757nm>0]\n",
    "    cloud_flag                     = cloud_flag                     [rSIF_757nm>0]\n",
    "    cloud_co2_ratio                = cloud_co2_ratio                [rSIF_757nm>0]\n",
    "    cloud_delta_surface_pressure   = cloud_delta_surface_pressure   [rSIF_757nm>0]\n",
    "    cloud_o2_ratio                 = cloud_o2_ratio                 [rSIF_757nm>0]\n",
    "    vapor_pressure_deficit         = vapor_pressure_deficit         [rSIF_757nm>0]\n",
    "    temperature_2m                 = temperature_2m                 [rSIF_757nm>0]\n",
    "    temperature_skin               = temperature_skin               [rSIF_757nm>0]\n",
    "    specific_humidity              = specific_humidity              [rSIF_757nm>0]\n",
    "    surface_pressure               = surface_pressure               [rSIF_757nm>0]\n",
    "    wind_speed                     = wind_speed                     [rSIF_757nm>0]\n",
    "    continuum_radiance_757nm       = continuum_radiance_757nm       [rSIF_757nm>0]\n",
    "    continuum_radiance_771nm       = continuum_radiance_771nm       [rSIF_757nm>0]\n",
    "    daily_correction_factor        = daily_correction_factor        [rSIF_757nm>0]\n",
    "    footprint                      = footprint                      [rSIF_757nm>0]\n",
    "    IGBP_index                     = IGBP_index                     [rSIF_757nm>0]\n",
    "    latitude                       = latitude                       [rSIF_757nm>0]\n",
    "    longitude                      = longitude                      [rSIF_757nm>0]\n",
    "    measurement_mode               = measurement_mode               [rSIF_757nm>0]\n",
    "    orbit_number                   = orbit_number                   [rSIF_757nm>0]\n",
    "    reduced_chi2_757nm             = reduced_chi2_757nm             [rSIF_757nm>0]\n",
    "    reduced_chi2_771nm             = reduced_chi2_771nm             [rSIF_757nm>0]\n",
    "    sensor_azimuth_angle           = sensor_azimuth_angle           [rSIF_757nm>0]\n",
    "    sensor_zenith_angle            = sensor_zenith_angle            [rSIF_757nm>0]\n",
    "    SIF_757nm_relative             = SIF_757nm_relative             [rSIF_757nm>0]\n",
    "    SIF_757nm_uncert               = SIF_757nm_uncert               [rSIF_757nm>0]\n",
    "    SIF_771nm                      = SIF_771nm                      [rSIF_757nm>0]\n",
    "    SIF_771nm_relative             = SIF_771nm_relative             [rSIF_757nm>0]\n",
    "    SIF_771nm_uncert               = SIF_771nm_uncert               [rSIF_757nm>0]\n",
    "    solar_azimuth_angle            = solar_azimuth_angle            [rSIF_757nm>0]\n",
    "    solar_zenith_angle             = solar_zenith_angle             [rSIF_757nm>0]\n",
    "    sounding_id                    = sounding_id                    [rSIF_757nm>0]\n",
    "    surface_altitude               = surface_altitude               [rSIF_757nm>0]\n",
    "    time                           = time                           [rSIF_757nm>0]\n",
    "    uncorrected_SIF_757nm          = uncorrected_SIF_757nm          [rSIF_757nm>0]\n",
    "    uncorrected_SIF_757nm_relative = uncorrected_SIF_757nm_relative [rSIF_757nm>0]\n",
    "    uncorrected_SIF_771nm          = uncorrected_SIF_771nm          [rSIF_757nm>0]\n",
    "    uncorrected_SIF_771nm_relative = uncorrected_SIF_771nm_relative [rSIF_757nm>0]\n",
    "      \n",
    "    sif_rows  = [(SIF_757nm[i], cloud_albedo[i], cloud_flag[i], cloud_co2_ratio[i], cloud_delta_surface_pressure[i], cloud_o2_ratio[i], vapor_pressure_deficit[i],\n",
    "                  temperature_2m[i], temperature_skin[i], specific_humidity[i], surface_pressure[i], wind_speed[i], continuum_radiance_757nm[i],\n",
    "                  continuum_radiance_771nm[i],daily_correction_factor[i], footprint[i], IGBP_index[i], latitude[i], longitude[i], measurement_mode[i],\n",
    "                  orbit_number[i], reduced_chi2_757nm[i],reduced_chi2_771nm[i], sensor_azimuth_angle[i], sensor_zenith_angle[i], SIF_757nm_relative[i],\n",
    "                  SIF_757nm_uncert[i], SIF_771nm[i],SIF_771nm_relative[i], SIF_771nm_uncert[i], solar_azimuth_angle[i], solar_zenith_angle[i], sounding_id[i],\n",
    "                  surface_altitude[i], time[i], uncorrected_SIF_757nm[i], uncorrected_SIF_757nm_relative[i], uncorrected_SIF_771nm[i], uncorrected_SIF_771nm_relative[i])\n",
    "                  for i in range(0,len(sounding_id))]\n",
    "    \n",
    "    column_labels = ['SIF_757nm', 'cloud_albedo', 'cloud_flag', 'cloud_co2_ratio', 'cloud_delta_surface_pressure', 'cloud_o2_ratio', 'vapor_pressure_deficit',\n",
    "                     'temperature_2m', 'temperature_skin', 'specific_humidity', 'surface_pressure', 'wind_speed', 'continuum_radiance_757nm',\n",
    "                     'continuum_radiance_771nm','daily_correction_factor', 'footprint', 'IGBP_index', 'latitude', 'longitude', 'measurement_mode',\n",
    "                     'orbit_number', 'reduced_chi2_757nm','reduced_chi2_771nm', 'sensor_azimuth_angle', 'sensor_zenith_angle',\n",
    "                     'SIF_757nm_relative', 'SIF_757nm_uncert', 'SIF_771nm','SIF_771nm_relative', 'SIF_771nm_uncert', 'solar_azimuth_angle', 'solar_zenith_angle',\n",
    "                     'sounding_id', 'surface_altitude', 'time','uncorrected_SIF_757nm', 'uncorrected_SIF_757nm_relative', 'uncorrected_SIF_771nm',\n",
    "                     'uncorrected_SIF_771nm_relative']\n",
    "     \n",
    "    sif_df                         = pd.DataFrame(sif_rows,columns = column_labels) #-------- Create sif variables' dataframe\n",
    "    \n",
    "    \n",
    "    calipso_sif_merger             = pd.merge(sif_df, calipso_df, on = ['sounding_id'], how = 'inner') #--------Merge sif and calipso on sounding _id\n",
    "\n",
    "    calipso_sif_merger['Date']     = calipso_sif_merger['sounding_id'].map(lambda x: '-'.join([str(x)[:4],str(x)[4:6],str(x)[6:]])[:10]) #-----Create new date column\n",
    "    calipso_sif_merger['SIF_Time'] = calipso_sif_merger['sounding_id'].map(lambda x: Times(x))  #----------------------------------------------Create new time column\n",
    "    calipso_sif_merger['tile_h'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[0], axis=1) #------Create new horizontal tile column\n",
    "    calipso_sif_merger['tile_v'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[1], axis=1) #------Create new vertical tile column\n",
    "    calipso_sif_merger             = calipso_sif_merger.dropna(how='any')\n",
    "    calipso_sif_merger.to_csv('Processed_sif/df_sif_{}.csv'.format(sif_date), index=False)\n",
    "    grp         = calipso_sif_merger.groupby(['tile_h', 'tile_v']).agg(lambda x: list(x))  #----Group sif-calipso merger(from now on called SIF*) by tile id\n",
    "    grp         = grp.reset_index() #-----------------------------------------------------------Reset indices\n",
    "    l_ungrouped = len(calipso_sif_merger)\n",
    "    l_grouped   = len(grp)\n",
    "    \n",
    "    df          = grp.copy() #--------make a copy of the grouped file\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    printmd('**For {}, there are {} sif footprints scattered over {} tiles.**'.format(sif_date, l_ungrouped, l_grouped))\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')  \n",
    "    \n",
    "    \n",
    "    # START PAR PROCESSING\n",
    "    \n",
    "    print('PAR Processing Started...\\n')\n",
    "    shape              = (240,240)\n",
    "    Each_Par_Tile_Data = []   #-------------Create empty list to store all the par extracted data as tuples\n",
    "    \n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'],df['sounding_id']): #Loop through rows\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')\n",
    "        \n",
    "        for folder_number in range(len(par_folder_list)): #-------------------------Go inside folders of daily par files\n",
    "            par_julian_day    = par_folder_list[folder_number].split('/')[1] #------Extract par julian day\n",
    "            \n",
    "            if sif_julian_day == par_julian_day: #------------------------------------If sif julian day is same as par folder julian day,\n",
    "                par_file_list = glob.glob(par_folder_list[folder_number]+'/*.hdf')#---open that folder (this folder contains par files for that day)\n",
    "            \n",
    "                for par_file in par_file_list: #--------------------------------------Loop through par files in par file list created in above line\n",
    "                    h_par = par_file.split('.')[2][1:3] #---------------Extract par h tile no.\n",
    "                    v_par = par_file.split('.')[2][4:6] #---------------Extract par v tile no.\n",
    "                    \n",
    "                    if (h_par==h_sif) and (v_par==v_sif): #-------------If sif tiles match with par tiles, open par file otherwise go to the next par file to check.\n",
    "                        par       = Dataset(par_file, mode='r')\n",
    "                        par_date  = datetime.datetime.strptime(par_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                        gmt_0000  = par.variables['GMT_0000_PAR'][:].flatten() #-----------------------------------------Read par 3-hourly variables\n",
    "                        gmt_0300  = par.variables['GMT_0300_PAR'][:].flatten()\n",
    "                        gmt_0600  = par.variables['GMT_0600_PAR'][:].flatten()\n",
    "                        gmt_0900  = par.variables['GMT_0900_PAR'][:].flatten()\n",
    "                        gmt_1200  = par.variables['GMT_1200_PAR'][:].flatten()\n",
    "                        gmt_1500  = par.variables['GMT_1500_PAR'][:].flatten()\n",
    "                        gmt_1800  = par.variables['GMT_1800_PAR'][:].flatten()\n",
    "                        gmt_2100  = par.variables['GMT_2100_PAR'][:].flatten()\n",
    "                        struct    = getattr(par, 'StructMetadata.0')\n",
    "                        struct1   = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3] #-------------]\n",
    "                        struct2   = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3] #-------------] Extract upper right and lower left\n",
    "                        ULx, ULy  = literal_eval(struct1) #-----------------------------------------------------------------------] coordinates of the tile of opened par file\n",
    "                        LRx, LRy  = literal_eval(struct2) #-----------------------------------------------------------------------]\n",
    "                        par_lon,par_lat = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape) #----------- Extract the par lat,lon meshgrid in proper projection \n",
    "                        tree      = spatial.KDTree(   list(  zip(par_lon, par_lat) )) #---------------- Create a nearest neighbor spatial tree\n",
    "\n",
    "                        for sub in range(len(sif_time)): #--------------------------Loop through the list of a dataframe cell (grouped dataframe cells consist of lists of values)\n",
    "                            target          = (sif_lat[sub] , sif_lon[sub])  #---------------------------------------------------Make sif coordinates target for spatial interpolation\n",
    "                            sif_time_sub    = re.sub(':', '', sif_time[sub]) #---------------------------------------------------Remove : from sif_time\n",
    "                            timeX           = pd.to_datetime(sif_time_sub.ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")#--Change resolution from SSSS to SS for faster interpolation\n",
    "                            lower_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[0]).zfill(8)[:4] #---Find lower bound of interval in which sif time lies\n",
    "                       \n",
    "                            if f(int(sif_time_sub))[1] == 23595900: #-----------------------------------------------If upper bound is greater than 2100 hours that is 235959\n",
    "                                upper_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[1]).zfill(8)[:6] #----upper key becomes interp_gmt_235959\n",
    "                            else:                                                                              #----else                   \n",
    "                                upper_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[1]).zfill(8)[:4] #----upper key is as it is (interp_gmt_1200,1500,1800,etc)\n",
    "                        \n",
    "                            neigh5          = tree.query([(sif_lon[sub], sif_lat[sub])], k=3)[1][0] #---Find k=3 nearest spatial neighbors indices at target\n",
    "                            lon_for_idw     = [par_lon[i] for i in neigh5] #----------------------------Extract longitudes at these indices\n",
    "                            lat_for_idw     = [par_lat[i] for i in neigh5] #----------------------------Extract latitudes also\n",
    "                            coors_for_idw   = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] #---------Create a list of coordinates (lat lon tuples)\n",
    "\n",
    "                            gmt_0000n       = [gmt_0000[i] for i in neigh5] #---------------------------Find variable values at nearest neighbors\n",
    "                            gmt_0300n       = [gmt_0300[i] for i in neigh5]\n",
    "                            gmt_0600n       = [gmt_0600[i] for i in neigh5]\n",
    "                            gmt_0900n       = [gmt_0900[i] for i in neigh5]\n",
    "                            gmt_1200n       = [gmt_1200[i] for i in neigh5]\n",
    "                            gmt_1500n       = [gmt_1500[i] for i in neigh5]\n",
    "                            gmt_1800n       = [gmt_1800[i] for i in neigh5]\n",
    "                            gmt_2100n       = [gmt_2100[i] for i in neigh5]\n",
    "\n",
    "                            func_gmt_0000   = idw(coors_for_idw, gmt_0000n) #---------------------------Create inverse distance weighing (IDW) interpolation function at nn coordinates\n",
    "                            func_gmt_0300   = idw(coors_for_idw, gmt_0300n)\n",
    "                            func_gmt_0600   = idw(coors_for_idw, gmt_0600n)\n",
    "                            func_gmt_0900   = idw(coors_for_idw, gmt_0900n)\n",
    "                            func_gmt_1200   = idw(coors_for_idw, gmt_1200n)\n",
    "                            func_gmt_1500   = idw(coors_for_idw, gmt_1500n)\n",
    "                            func_gmt_1800   = idw(coors_for_idw, gmt_1800n)\n",
    "                            func_gmt_2100   = idw(coors_for_idw, gmt_2100n)\n",
    "\n",
    "                            interp_gmt_0000 = func_gmt_0000(target) #-----------------------------------Find interpolated hourly par at target\n",
    "                            interp_gmt_0300 = func_gmt_0300(target)\n",
    "                            interp_gmt_0600 = func_gmt_0600(target)\n",
    "                            interp_gmt_0900 = func_gmt_0900(target)\n",
    "                            interp_gmt_1200 = func_gmt_1200(target)\n",
    "                            interp_gmt_1500 = func_gmt_1500(target)\n",
    "                            interp_gmt_1800 = func_gmt_1800(target)\n",
    "                            interp_gmt_2100 = func_gmt_2100(target)\n",
    "\n",
    "                            interp_gmt_235959 = 0\n",
    "                            mydict = {'interp_gmt_0000':interp_gmt_0000, 'interp_gmt_0300':interp_gmt_0300, 'interp_gmt_0600'  :interp_gmt_0600,\n",
    "                                      'interp_gmt_0900':interp_gmt_0900, 'interp_gmt_1200':interp_gmt_1200, 'interp_gmt_1500'  :interp_gmt_1500,\n",
    "                                      'interp_gmt_1800':interp_gmt_1800, 'interp_gmt_2100':interp_gmt_2100, 'interp_gmt_235959':interp_gmt_235959} #---Save variables in dictionary \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                            lower_bound_value = mydict[lower_bound_key] #-----Find lower 3 hourly par value\n",
    "                            upper_bound_value = mydict[upper_bound_key] #-----Find upper 3 hourly par value\n",
    "\n",
    "                            if upper_bound_value == interp_gmt_235959:  #---------------------------- If upper par value > 2100, we need to extract its value from next day par file\n",
    "                                par_file_list = glob.glob(par_folder_list[folder_number+1]+'/*.hdf') #---Open next day par file list\n",
    "\n",
    "                                for par_file in par_file_list:  #----------------------------]\n",
    "                                    h_par = par_file.split('.')[2][1:3]#---------------------] Check for date and tile\n",
    "                                    v_par = par_file.split('.')[2][4:6]#---------------------]\n",
    "\n",
    "                                    if (h_par == h_sif) and (v_par == v_sif): #-----------------------------------------] Repeat above procedure\n",
    "                                        par = Dataset(par_file, mode='r') #---------------------------------------------] once again just\n",
    "                                        gmt_235959        = par.variables['GMT_0000_PAR'][:].flatten() #----------------] to obtain the spatially\n",
    "                                        lon_for_idw       = [par_lon[i] for i in neigh5] #------------------------------] interpolated par \n",
    "                                        lat_for_idw       = [par_lat[i] for i in neigh5] #------------------------------] value at the upper bound\n",
    "                                        coors_for_idw     = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] #-----------]\n",
    "                                        gmt_235959        = [gmt_235959[i] for i in neigh5] #---------------------------]\n",
    "                                        func_gmt_235959   = idw(coors_for_idw, gmt_235959) #----------------------------]\n",
    "                                        interp_gmt_235959 = func_gmt_235959(target) #-----------------------------------]\n",
    "                                        break\n",
    "                                upper_bound_value = interp_gmt_235959\n",
    "                            \n",
    "                            # Do TIME INTERPOLATION\n",
    "                            time1a = pd.to_datetime(lower_bound_key[11:].ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "                            time2a = pd.to_datetime(upper_bound_key[11:].ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "                            val1   = lower_bound_value\n",
    "                            val2   = upper_bound_value\n",
    "                            valX   = temporal_interpolation(time1a,val1,time2a,val2,timeX)\n",
    "                            Each_Par_Tile_Data.append((sif_sid[sub],sif_lat[sub],sif_lon[sub],valX)) #--------Add sif sounding id, sif lat, sif lon and final interpolated value to list\n",
    "    \n",
    "    DF = pd.DataFrame(np.array(Each_Par_Tile_Data), columns=['sounding_id','latitude','longitude', 'par']) #---------------Write each list of par data in a dataframe\n",
    "    DF.to_csv('Processed_par/df_par_{}.csv'.format(sif_date),index=False)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print('REF Processing Started...\\n')\n",
    "    shape2              = (2400,2400)\n",
    "    Each_Ref_Tile_Data  = []\n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'],df['sounding_id']):\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')\n",
    "        \n",
    "        for folder_number in range(len(ref_folder_list)):\n",
    "            ref_julian_day    = ref_folder_list[folder_number].split('/')[1]\n",
    "            \n",
    "            if sif_julian_day == ref_julian_day:\n",
    "                ref_file_list = glob.glob(ref_folder_list[folder_number]+'/*.hdf')\n",
    "            \n",
    "                for num3,ref_file in enumerate(ref_file_list):\n",
    "                    h_ref = ref_file.split('.')[2][1:3]\n",
    "                    v_ref = ref_file.split('.')[2][4:6]\n",
    "                    \n",
    "                    if (h_ref==h_sif) and (v_ref==v_sif):\n",
    "                        ref_date   = datetime.datetime.strptime(ref_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                        ref        = Dataset(ref_file, mode='r')\n",
    "                        nrb1       = ref.variables['Nadir_Reflectance_Band1'][:].flatten() \n",
    "                        nrb2       = ref.variables['Nadir_Reflectance_Band2'][:].flatten()\n",
    "                        struct     = getattr(ref, 'StructMetadata.0')\n",
    "                        struct1    = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3]\n",
    "                        struct2    = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3]\n",
    "                        ULx, ULy   = literal_eval(struct1)\n",
    "                        LRx, LRy   = literal_eval(struct2)\n",
    "                        ref_lon,ref_lat  = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape2)\n",
    "                        tree       = spatial.KDTree( list(  zip(ref_lon, ref_lat) ))\n",
    "                        \n",
    "                        for sub in range(len(sif_time)):\n",
    "                            target     = (sif_lat[sub] , sif_lon[sub])\n",
    "                            ind        = tree.query([(sif_lon[sub],sif_lat[sub])], k=1)[1][0]\n",
    "                            val1, val2 = nrb1[ind], nrb2[ind] \n",
    "                            Each_Ref_Tile_Data.append((sif_sid[sub], sif_lat[sub],sif_lon[sub],nrb1[ind],nrb2[ind]))\n",
    "                \n",
    "                        \n",
    "    DG = pd.DataFrame(np.array(Each_Ref_Tile_Data),columns=['sounding_id','latitude','longitude','nrb1','nrb2'])\n",
    "    DG.to_csv('Processed_ref/df_ref_{}.csv'.format(sif_date),index=False)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print('FPAR Processing Started...\\n')\n",
    "    shape3               = (2400,2400)\n",
    "    Each_fpar_Tile_Data  = []\n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'], df['sounding_id']):\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')            \n",
    "        for num4,fpar_file in enumerate(fpar_file_list):\n",
    "            h_fpar = fpar_file.split('.')[2][1:3]\n",
    "            v_fpar = fpar_file.split('.')[2][4:6]\n",
    "\n",
    "            if (h_fpar==h_sif) and (v_fpar==v_sif):\n",
    "                fpar_date     = datetime.datetime.strptime(fpar_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                fpar          = Dataset(fpar_file, mode='r')\n",
    "                fpar500       = fpar.variables['Fpar_500m'][:].flatten()\n",
    "                lai500        = fpar.variables['Lai_500m'][:].flatten()                \n",
    "                struct        = getattr(fpar, 'StructMetadata.0')\n",
    "                struct1       = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3]\n",
    "                struct2       = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3]\n",
    "                ULx, ULy      = literal_eval(struct1)\n",
    "                LRx, LRy      = literal_eval(struct2)\n",
    "                fpar_lon,fpar_lat  = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape3)\n",
    "                tree          = spatial.KDTree( list(  zip(fpar_lon, fpar_lat) ))\n",
    "\n",
    "                for sub in range(len(sif_time)):\n",
    "                    target     = (sif_lat[sub] , sif_lon[sub])\n",
    "                    ind        = tree.query([(sif_lon[sub],sif_lat[sub])], k=1)[1][0]\n",
    "                    Each_fpar_Tile_Data.append((sif_sid[sub],sif_lat[sub],sif_lon[sub],fpar500[ind],lai500[ind]))\n",
    "                \n",
    "                        \n",
    "    DH = pd.DataFrame(np.array(Each_fpar_Tile_Data),columns=['sounding_id','latitude','longitude','Fpar_500m','Lai_500m'])\n",
    "    DH.to_csv('Processed_fpar/df_fpar_{}.csv'.format(sif_date),index=False)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "sif_list  = sorted(glob.glob('Processed_sif/*.csv' ))\n",
    "par_list  = sorted(glob.glob('Processed_par/*.csv' ))\n",
    "ref_list  = sorted(glob.glob('Processed_ref/*.csv' ))\n",
    "fpar_list = sorted(glob.glob('Processed_fpar/*.csv'))\n",
    "\n",
    "for i_sif, i_par, i_ref, i_fpar in zip(sif_list, par_list, ref_list, fpar_list):\n",
    "\n",
    "    di  = pd.read_csv(i_sif)\n",
    "    dj  = pd.read_csv(i_par)\n",
    "    dk  = pd.read_csv(i_ref)\n",
    "    dl  = pd.read_csv(i_fpar)\n",
    "    dM1 = di .merge(dj, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "    dM2 = dM1.merge(dk, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "    dM3 = dM2.merge(dl, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "    dM3 = dM3.dropna(subset=['par','nrb1','nrb2','Fpar_500m','Lai_500m'])\n",
    "    L   = len(dM3)\n",
    "    print('Dimensions:',L)\n",
    "    dM3 = dM3.rename({'par':'PAR', 'nrb1':'Nadir_Reflectance_Band1', 'nrb2':'Nadir_Reflectance_Band2'}, axis=1)\n",
    "\n",
    "    with Dataset('OCO2_sif/oco2_LtSIF_180501_B8100r_180703004855s.nc4','r') as src_sif,\\\n",
    "    Dataset(\"Data%s.nc4\"%i_sif[21:-4], \"w\")                                 as output ,\\\n",
    "    Dataset('MCD18A2/121/MCD18A2.A2018121.h01v08.006.2019091180956.hdf')    as src_par,\\\n",
    "    Dataset('MCD43A4/121/MCD43A4.A2018121.h00v08.006.2018130031808.hdf')    as src_ref,\\\n",
    "    Dataset('MCD15A3H/MCD15A3H.A2018121.h00v08.006.2018129231051.hdf'  )    as src_fpar:\n",
    "        \n",
    "        group_sif     = output.createGroup(\"Group_OCO2\"       )\n",
    "        group_par     = output.createGroup(\"Group_MCD18A2\"    )\n",
    "        group_ref     = output.createGroup(\"Group_MCD43A4\"    )\n",
    "        group_fpar    = output.createGroup(\"Group_MCD15A3H\"   )\n",
    "        \n",
    "        output.setncatts(src_sif.__dict__)\n",
    "        output.setncatts(src_par.__dict__)\n",
    "        output.setncatts(src_ref.__dict__)\n",
    "        output.setncatts(src_fpar.__dict__)\n",
    "        \n",
    "        for dfname in dM3.columns:\n",
    "            \n",
    "            for name, variable in src_sif.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_OCO2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_sif[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_par.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD18A2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_par[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "                    \n",
    "            for name, variable in src_ref.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD43A4/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_ref[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_fpar.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD15A3H/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_fpar[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
