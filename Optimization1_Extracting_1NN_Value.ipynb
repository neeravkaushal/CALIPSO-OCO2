{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell  #---- Output all jupyter lab inputs instead of the last one\n",
    "from IPython.display import Markdown, display\n",
    "InteractiveShell.ast_node_interactivity = \"all\"             \n",
    "import glob #---------------------------------------------------- To read the files or folders in a system directory\n",
    "from netCDF4 import Dataset #------------------------------------ To read nc , nc4 and hdf4 files\n",
    "import numpy as np\n",
    "import datetime\n",
    "import h5py #---------------------------------------------------- To read hdf5 files\n",
    "from scipy import spatial #-------------------------------------- To extract the values and indices of k nearest neighbors\n",
    "import pandas as pd\n",
    "from ast import literal_eval #----------------------------------- For literal evaluation of a string to extract python objects\n",
    "from pyproj import Proj, transform #----------------------------- To interconvert different projections\n",
    "import warnings #------------------------------------------------ To suppress warnings\n",
    "from photutils.utils import ShepardIDWInterpolator as idw #------ To use Shepard's Inverse Distance Weighing Interpolation tool\n",
    "import re #------------------------------------------------------ To replace characters in a string\n",
    "import time as tttt\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter('ignore')\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_proj  = Proj('+proj=sinu +R=6371007.181 +nadgrids=@null +wktext') #------ Specify input projection\n",
    "out_proj = Proj(init='epsg:4326') #----------------------------------------- Specify output projection\n",
    "def Times(x): #------------------------------------------Extract Time from sounding ID. NOTE the time format is HH:MM:SSSS\n",
    "    y   = str(x)\n",
    "    yy  = y[8:]\n",
    "    yyy = '{}:{}:{}'.format(yy[:2], yy[2:4], yy[4:])\n",
    "    return yyy\n",
    "hours = [0, 3000000, 6000000, 9000000, 12000000, 15000000, 18000000, 21000000, 23595900]\n",
    "def f(x): #---------------------------------------------Extract the hour interval of sif time (which  is in seconds)\n",
    "    for i in range(len(hours)):\n",
    "        if (x>hours[i]) and (x<hours[i+1]):\n",
    "            lb = hours[i]\n",
    "            ub = hours[i+1]\n",
    "            return lb,ub\n",
    "            break\n",
    "def format_time(t): #----------------------------------- Format the time into HH:MM:SSSS for the raw format HH:MM:SSSSSSSS\n",
    "    s = t\n",
    "    return s[:-4]\n",
    "def nn(latitude_list,longitude_list,target): #---------- Find the index of nearest neighbor (NOTE: absolute difference)\n",
    "    target_lat, target_lon = target[1], target[0]\n",
    "    d = [abs(latitude-target_lat) + abs(longitude-target_lon) for latitude,longitude in zip(latitude_list,longitude_list)]\n",
    "    return np.argmin(d)\n",
    "data = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3)\n",
    "def tile_finder(Lat,Lon): #----------------------------- Find modis tile numbers in which the argument lat,lon lies\n",
    "    in_tile = False\n",
    "    i = 0\n",
    "    while(not in_tile):\n",
    "        in_tile = Lat >= data[i, 4] and Lat <= data[i, 5] and Lon >= data[i, 2] and Lon <= data[i, 3]\n",
    "        i += 1\n",
    "    V = str(int(data[i-1, 0])).zfill(2)\n",
    "    H = str(int(data[i-1, 1])).zfill(2)\n",
    "    return H,V\n",
    "def extract_pixel_coordinates(ULx,Uly,LRx,LRy,shape):\n",
    "    x        = np.linspace(ULx, LRx, shape[0], endpoint=False) + abs((ULx-LRx)/(2*shape[0]))\n",
    "    y        = np.linspace(ULy, LRy, shape[0], endpoint=False) - abs((ULy-LRy)/(2*shape[0]))\n",
    "    xx, yy   = np.meshgrid(x,y)\n",
    "    xs       = xx.flatten()\n",
    "    ys       = yy.flatten()\n",
    "    plon, plat = transform(in_proj, out_proj, xs, ys)\n",
    "    return plon, plat\n",
    "def temporal_interpolation(time1,val1,time2,val2,timeX):\n",
    "    df    = pd.DataFrame( [(time1, val1) , (time2, val2)] , columns=['Times','Values'] ) \n",
    "    df    = df.set_index('Times')\n",
    "    df    = pd.Series(df['Values'], index=df.index)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    inter = df.resample('S').interpolate(method='linear')\n",
    "    valX  = inter.loc[timeX]\n",
    "    return valX\n",
    "sif_file_list     = glob.glob('OCO2_sif/*.nc4')    #--------------------------------------------- List of all OCO2 files\n",
    "calipso_file_list = glob.glob('OCO2_calipso/*.h5') #--------------------------------------------- List of all OCO2-CALIPSO files\n",
    "fpar_file_list    = glob.glob('MCD15A3H/*.hdf')    \n",
    "par_folder_list   = glob.glob('MCD18A2/*')\n",
    "ref_folder_list   = glob.glob('MCD43A4/*')\n",
    "data              = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3) #------ File having tile numbers and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sif_file in sif_file_list: #---------------------------------------------------------------------------Read one sif file at a time\n",
    "    sif_date        = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%Y-%m-%d\") #-----Extract sif date\n",
    "    sif_julian_day  = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%j\") #-----------Extract sif julian day\n",
    "    sif             = Dataset(sif_file, mode='r') #-----------------------------------------------------------Open sif file\n",
    "    calipso_df_list = [] #------------------------------------------------------------------------------------Create an empty list\n",
    "    for calipso_file in calipso_file_list: #------------------------------------------------------------------Loop through all calipso files\n",
    "        calipso_date      = datetime.datetime.strptime(calipso_file.split('_')[5], '%y%m%d').strftime(\"%Y-%m-%d\")\n",
    "        if calipso_date  == sif_date: #----------------------------------------------------------------------------If calipso date matches sif date,\n",
    "            calipso       = h5py.File(calipso_file, mode='r') #----------------------------------------------------open the calipso file\n",
    "            calipso_ID    = calipso['OCO2_sounding_id'                                           ][:]\n",
    "            calipso_dist  = calipso['matchup_distance_km'                                        ][:]\n",
    "            calipso_index = calipso['matchup_Xindex'                                             ][:]\n",
    "            calipso_dfs   = pd.DataFrame({'sounding_id':calipso_ID.flatten(),'Xindex':calipso_index.flatten(),'Xdistance':calipso_dist.flatten()}) #-----Create dataframe with variables\n",
    "            calipso_dfs[calipso_dfs.Xindex==-999.0] = np.nan #------- Replace missing values with nan\n",
    "            calipso_dfs.dropna(inplace=True) #----------------------- Drop missing values of Xindex\n",
    "            calipso_dfs[calipso_dfs.Xdistance>=2.0] = np.nan\n",
    "            calipso_dfs.dropna(inplace=True)\n",
    "            calipso_df_list.append(calipso_dfs) #-------------------- Add all calipso dataframes into a list\n",
    "    calipso_df                     = pd.concat(calipso_df_list, ignore_index = True).drop_duplicates() #---------Create a final calipso dataframe for a day\n",
    "\n",
    "    cloud_albedo                   = sif.groups['Cloud'].variables['albedo'                 ][:].flatten() #--------Read sif variables and flatten them\n",
    "    cloud_flag                     = sif.groups['Cloud'].variables['cloud_flag'             ][:].flatten()\n",
    "    cloud_co2_ratio                = sif.groups['Cloud'].variables['co2_ratio'              ][:].flatten()\n",
    "    cloud_delta_surface_pressure   = sif.groups['Cloud'].variables['delta_surface_pressure' ][:].flatten()\n",
    "    cloud_o2_ratio                 = sif.groups['Cloud'].variables['o2_ratio'               ][:].flatten()\n",
    "    vapor_pressure_deficit         = sif.groups['Meteo'].variables['vapor_pressure_deficit' ][:].flatten()\n",
    "    temperature_2m                 = sif.groups['Meteo'].variables['2m_temperature'         ][:].flatten()\n",
    "    temperature_skin               = sif.groups['Meteo'].variables['skin_temperature'       ][:].flatten()\n",
    "    specific_humidity              = sif.groups['Meteo'].variables['specific_humidity'      ][:].flatten()\n",
    "    surface_pressure               = sif.groups['Meteo'].variables['surface_pressure'       ][:].flatten()\n",
    "    wind_speed                     = sif.groups['Meteo'].variables['wind_speed'             ][:].flatten()\n",
    "    continuum_radiance_757nm       = sif.variables         ['continuum_radiance_757nm'      ][:].flatten()\n",
    "    continuum_radiance_771nm       = sif.variables         ['continuum_radiance_771nm'      ][:].flatten()\n",
    "    daily_correction_factor        = sif.variables         ['daily_correction_factor'       ][:].flatten()\n",
    "    footprint                      = sif.variables         ['footprint'                     ][:].flatten()\n",
    "    IGBP_index                     = sif.variables         ['IGBP_index'                    ][:].flatten()\n",
    "    latitude                       = sif.variables         ['latitude'                      ][:].flatten()\n",
    "    longitude                      = sif.variables         ['longitude'                     ][:].flatten()\n",
    "    measurement_mode               = sif.variables         ['measurement_mode'              ][:].flatten()\n",
    "    orbit_number                   = sif.variables         ['orbit_number'                  ][:].flatten()\n",
    "    reduced_chi2_757nm             = sif.variables         ['reduced_chi2_757nm'            ][:].flatten()\n",
    "    reduced_chi2_771nm             = sif.variables         ['reduced_chi2_771nm'            ][:].flatten()\n",
    "    sensor_azimuth_angle           = sif.variables         ['sensor_azimuth_angle'          ][:].flatten()\n",
    "    sensor_zenith_angle            = sif.variables         ['sensor_zenith_angle'           ][:].flatten()\n",
    "    rSIF_757nm                     = sif.variables         ['SIF_757nm'                     ][:].flatten() #----------Use this condition for next block\n",
    "    SIF_757nm_relative             = sif.variables         ['SIF_757nm_relative'            ][:].flatten()\n",
    "    SIF_757nm_uncert               = sif.variables         ['SIF_757nm_uncert'              ][:].flatten()\n",
    "    SIF_771nm                      = sif.variables         ['SIF_771nm'                     ][:].flatten()\n",
    "    SIF_771nm_relative             = sif.variables         ['SIF_771nm_relative'            ][:].flatten()\n",
    "    SIF_771nm_uncert               = sif.variables         ['SIF_771nm_uncert'              ][:].flatten()\n",
    "    solar_azimuth_angle            = sif.variables         ['solar_azimuth_angle'           ][:].flatten()\n",
    "    solar_zenith_angle             = sif.variables         ['solar_zenith_angle'            ][:].flatten()\n",
    "    sounding_id                    = sif.variables         ['sounding_id'                   ][:].flatten()\n",
    "    surface_altitude               = sif.variables         ['surface_altitude'              ][:].flatten()\n",
    "    time                           = sif.variables         ['time'                          ][:].flatten()\n",
    "    uncorrected_SIF_757nm          = sif.variables         ['uncorrected_SIF_757nm'         ][:].flatten()\n",
    "    uncorrected_SIF_757nm_relative = sif.variables         ['uncorrected_SIF_757nm_relative'][:].flatten()\n",
    "    uncorrected_SIF_771nm          = sif.variables         ['uncorrected_SIF_771nm'         ][:].flatten()\n",
    "    uncorrected_SIF_771nm_relative = sif.variables         ['uncorrected_SIF_771nm_relative'][:].flatten()\n",
    "    \n",
    "    \n",
    "    SIF_757nm                      = rSIF_757nm                     [rSIF_757nm>0]  #--------- Select only those values for which SIF_757nm > 0\n",
    "    cloud_albedo                   = cloud_albedo                   [rSIF_757nm>0]\n",
    "    cloud_flag                     = cloud_flag                     [rSIF_757nm>0]\n",
    "    cloud_co2_ratio                = cloud_co2_ratio                [rSIF_757nm>0]\n",
    "    cloud_delta_surface_pressure   = cloud_delta_surface_pressure   [rSIF_757nm>0]\n",
    "    cloud_o2_ratio                 = cloud_o2_ratio                 [rSIF_757nm>0]\n",
    "    vapor_pressure_deficit         = vapor_pressure_deficit         [rSIF_757nm>0]\n",
    "    temperature_2m                 = temperature_2m                 [rSIF_757nm>0]\n",
    "    temperature_skin               = temperature_skin               [rSIF_757nm>0]\n",
    "    specific_humidity              = specific_humidity              [rSIF_757nm>0]\n",
    "    surface_pressure               = surface_pressure               [rSIF_757nm>0]\n",
    "    wind_speed                     = wind_speed                     [rSIF_757nm>0]\n",
    "    continuum_radiance_757nm       = continuum_radiance_757nm       [rSIF_757nm>0]\n",
    "    continuum_radiance_771nm       = continuum_radiance_771nm       [rSIF_757nm>0]\n",
    "    daily_correction_factor        = daily_correction_factor        [rSIF_757nm>0]\n",
    "    footprint                      = footprint                      [rSIF_757nm>0]\n",
    "    IGBP_index                     = IGBP_index                     [rSIF_757nm>0]\n",
    "    latitude                       = latitude                       [rSIF_757nm>0]\n",
    "    longitude                      = longitude                      [rSIF_757nm>0]\n",
    "    measurement_mode               = measurement_mode               [rSIF_757nm>0]\n",
    "    orbit_number                   = orbit_number                   [rSIF_757nm>0]\n",
    "    reduced_chi2_757nm             = reduced_chi2_757nm             [rSIF_757nm>0]\n",
    "    reduced_chi2_771nm             = reduced_chi2_771nm             [rSIF_757nm>0]\n",
    "    sensor_azimuth_angle           = sensor_azimuth_angle           [rSIF_757nm>0]\n",
    "    sensor_zenith_angle            = sensor_zenith_angle            [rSIF_757nm>0]\n",
    "    SIF_757nm_relative             = SIF_757nm_relative             [rSIF_757nm>0]\n",
    "    SIF_757nm_uncert               = SIF_757nm_uncert               [rSIF_757nm>0]\n",
    "    SIF_771nm                      = SIF_771nm                      [rSIF_757nm>0]\n",
    "    SIF_771nm_relative             = SIF_771nm_relative             [rSIF_757nm>0]\n",
    "    SIF_771nm_uncert               = SIF_771nm_uncert               [rSIF_757nm>0]\n",
    "    solar_azimuth_angle            = solar_azimuth_angle            [rSIF_757nm>0]\n",
    "    solar_zenith_angle             = solar_zenith_angle             [rSIF_757nm>0]\n",
    "    sounding_id                    = sounding_id                    [rSIF_757nm>0]\n",
    "    surface_altitude               = surface_altitude               [rSIF_757nm>0]\n",
    "    time                           = time                           [rSIF_757nm>0]\n",
    "    uncorrected_SIF_757nm          = uncorrected_SIF_757nm          [rSIF_757nm>0]\n",
    "    uncorrected_SIF_757nm_relative = uncorrected_SIF_757nm_relative [rSIF_757nm>0]\n",
    "    uncorrected_SIF_771nm          = uncorrected_SIF_771nm          [rSIF_757nm>0]\n",
    "    uncorrected_SIF_771nm_relative = uncorrected_SIF_771nm_relative [rSIF_757nm>0]\n",
    "      \n",
    "    sif_rows  = [(SIF_757nm[i], cloud_albedo[i], cloud_flag[i], cloud_co2_ratio[i], cloud_delta_surface_pressure[i], cloud_o2_ratio[i], vapor_pressure_deficit[i],\n",
    "                  temperature_2m[i], temperature_skin[i], specific_humidity[i], surface_pressure[i], wind_speed[i], continuum_radiance_757nm[i],\n",
    "                  continuum_radiance_771nm[i],daily_correction_factor[i], footprint[i], IGBP_index[i], latitude[i], longitude[i], measurement_mode[i],\n",
    "                  orbit_number[i], reduced_chi2_757nm[i],reduced_chi2_771nm[i], sensor_azimuth_angle[i], sensor_zenith_angle[i], SIF_757nm_relative[i],\n",
    "                  SIF_757nm_uncert[i], SIF_771nm[i],SIF_771nm_relative[i], SIF_771nm_uncert[i], solar_azimuth_angle[i], solar_zenith_angle[i], sounding_id[i],\n",
    "                  surface_altitude[i], time[i], uncorrected_SIF_757nm[i], uncorrected_SIF_757nm_relative[i], uncorrected_SIF_771nm[i], uncorrected_SIF_771nm_relative[i])\n",
    "                  for i in range(0,len(sounding_id))]\n",
    "    \n",
    "    column_labels = ['SIF_757nm', 'cloud_albedo', 'cloud_flag', 'cloud_co2_ratio', 'cloud_delta_surface_pressure', 'cloud_o2_ratio', 'vapor_pressure_deficit',\n",
    "                     'temperature_2m', 'temperature_skin', 'specific_humidity', 'surface_pressure', 'wind_speed', 'continuum_radiance_757nm',\n",
    "                     'continuum_radiance_771nm','daily_correction_factor', 'footprint', 'IGBP_index', 'latitude', 'longitude', 'measurement_mode',\n",
    "                     'orbit_number', 'reduced_chi2_757nm','reduced_chi2_771nm', 'sensor_azimuth_angle', 'sensor_zenith_angle',\n",
    "                     'SIF_757nm_relative', 'SIF_757nm_uncert', 'SIF_771nm','SIF_771nm_relative', 'SIF_771nm_uncert', 'solar_azimuth_angle', 'solar_zenith_angle',\n",
    "                     'sounding_id', 'surface_altitude', 'time','uncorrected_SIF_757nm', 'uncorrected_SIF_757nm_relative', 'uncorrected_SIF_771nm',\n",
    "                     'uncorrected_SIF_771nm_relative']\n",
    "     \n",
    "    sif_df                         = pd.DataFrame(sif_rows,columns = column_labels) #-------- Create sif variables' dataframe\n",
    "    calipso_sif_merger             = pd.merge(sif_df, calipso_df, on = ['sounding_id'], how = 'inner') #--------Merge sif and calipso on sounding _id\n",
    "    calipso_sif_merger['Date']     = calipso_sif_merger['sounding_id'].map(lambda x: '-'.join([str(x)[:4],str(x)[4:6],str(x)[6:]])[:10]) #-----Create new date column\n",
    "    calipso_sif_merger['SIF_Time'] = calipso_sif_merger['sounding_id'].map(lambda x: Times(x))  #----------------------------------------------Create new time column\n",
    "    calipso_sif_merger['tile_h'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[0], axis=1) #------Create new horizontal tile column\n",
    "    calipso_sif_merger['tile_v'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[1], axis=1) #------Create new vertical tile column\n",
    "    calipso_sif_merger             = calipso_sif_merger.dropna(how='any')\n",
    "    #calipso_sif_merger.to_csv('Processed_sif/df_sif_{}.csv'.format(sif_date), index=False)\n",
    "    grp         = calipso_sif_merger.groupby(['tile_h', 'tile_v']).agg(lambda x: list(x))  #----Group sif-calipso merger(from now on called SIF*) by tile id\n",
    "    grp         = grp.reset_index() #-----------------------------------------------------------Reset indices\n",
    "    l_ungrouped = len(calipso_sif_merger)\n",
    "    l_grouped   = len(grp)\n",
    "    df          = grp.copy() #--------make a copy of the grouped file\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    printmd('**For {}, there are {} sif footprints scattered over {} tiles.**'.format(sif_date, l_ungrouped, l_grouped))\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')  \n",
    "    \n",
    "\n",
    "    shapes              = [(2400,2400), (1200,1200), (800,800), (600,600), (400,400), (200,200)]\n",
    "    for shape2 in shapes:\n",
    "        print('Processing shape : ', shape2[0],'x',shape2[1])\n",
    "        a                   = tttt.time()\n",
    "        print('REF Processing Started...\\n')\n",
    "        Each_Ref_Tile_Data  = []\n",
    "\n",
    "        for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'],df['sounding_id']):\n",
    "            print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')\n",
    "\n",
    "            for folder_number in range(len(ref_folder_list)):\n",
    "                ref_julian_day    = ref_folder_list[folder_number].split('/')[1]\n",
    "\n",
    "                if sif_julian_day == ref_julian_day:\n",
    "                    ref_file_list = glob.glob(ref_folder_list[folder_number]+'/*.hdf')\n",
    "\n",
    "                    for num3,ref_file in enumerate(ref_file_list):\n",
    "                        h_ref = ref_file.split('.')[2][1:3]\n",
    "                        v_ref = ref_file.split('.')[2][4:6]\n",
    "\n",
    "                        if (h_ref==h_sif) and (v_ref==v_sif):\n",
    "                            ref_date         = datetime.datetime.strptime(ref_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                            ref              = Dataset(ref_file, mode='r')\n",
    "                            nrb1             = ref.variables['Nadir_Reflectance_Band1'][:].flatten() \n",
    "                            nrb2             = ref.variables['Nadir_Reflectance_Band2'][:].flatten()\n",
    "                            struct           = getattr(ref, 'StructMetadata.0')\n",
    "                            struct1          = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3]\n",
    "                            struct2          = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3]\n",
    "                            ULx, ULy         = literal_eval(struct1)\n",
    "                            LRx, LRy         = literal_eval(struct2)\n",
    "                            ref_lon,ref_lat  = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape2)\n",
    "                            tree             = spatial.KDTree( list(  zip(ref_lon, ref_lat) ))\n",
    "\n",
    "                            for sub in range(len(sif_time)):\n",
    "                                target       = (sif_lat[sub] , sif_lon[sub])\n",
    "                                ind          = tree.query([(sif_lon[sub],sif_lat[sub])], k=1)[1][0]\n",
    "                                val1, val2   = nrb1[ind], nrb2[ind]\n",
    "\n",
    "                                Each_Ref_Tile_Data.append((sif_sid[sub], sif_lat[sub],sif_lon[sub],nrb1[ind],nrb2[ind]))\n",
    "\n",
    "\n",
    "        DG = pd.DataFrame(np.array(Each_Ref_Tile_Data),columns=['sounding_id','latitude','longitude','nrb1','nrb2'])\n",
    "        DG.to_csv('Optimization/{}_ref_{}.csv'.format(sif_date,shape2[0]),index=False)\n",
    "        print('\\n')\n",
    "        b  = tttt.time()\n",
    "        printmd('**{} minutes**'.format((b-a)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day               = '03'\n",
    "df                = pd.read_csv('Optimization/2018-05-{}_ref_2400.csv'.format(day))\n",
    "df['sounding_id'] = df['sounding_id'].map(lambda x: int(x))\n",
    "\n",
    "list_of_refs = sorted(glob.glob('Optimization/2018-05-{}_*.csv'.format(day)))\n",
    "list_of_refs.remove('Optimization/2018-05-{}_ref_2400.csv'.format(day))\n",
    "\n",
    "for i in list_of_refs:\n",
    "    dg                = pd.read_csv(i)\n",
    "    dg['sounding_id'] = dg['sounding_id'].map(lambda x: int(x))   \n",
    "    dg['1_p']         = (dg['nrb1'] - df['nrb1']) * 100 / df['nrb1']\n",
    "    dg['2_p']         = (dg['nrb2'] - df['nrb2']) * 100 / df['nrb2']\n",
    "    dg                = dg.rename({'1_p':'1_p_{}'.format(i[28:-4]), '2_p':'2_p_{}'.format(i[28:-4])}, axis=1)\n",
    "    dg.to_csv('Optimization/2018-05-{}_REF_{}.csv'.format(day,i[28:-4]), index=False)\n",
    "    del dg\n",
    "\n",
    "da = pd.read_csv('Optimization/2018-05-{}_REF_200.csv'.format(day) )\n",
    "db = pd.read_csv('Optimization/2018-05-{}_REF_400.csv'.format(day) )\n",
    "dc = pd.read_csv('Optimization/2018-05-{}_REF_600.csv'.format(day) )\n",
    "dd = pd.read_csv('Optimization/2018-05-{}_REF_800.csv'.format(day) )\n",
    "de = pd.read_csv('Optimization/2018-05-{}_REF_1200.csv'.format(day))\n",
    "df = pd.read_csv('Optimization/2018-05-{}_ref_2400.csv'.format(day))\n",
    "\n",
    "dx1 = df. merge(da, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "dx2 = dx1.merge(db, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "dx3 = dx2.merge(dc, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "dx4 = dx3.merge(dd, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "dx5 = dx4.merge(de, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "\n",
    "dx5['sounding_id'].nunique()\n",
    "dx6 = dx5[['sounding_id','1_p_200','2_p_200','1_p_400','2_p_400','1_p_600','2_p_600','1_p_800','2_p_800','1_p_1200','2_p_1200',]]\n",
    "dx6 = dx6.replace([np.inf, -np.inf], np.nan).dropna(how=\"any\")\n",
    "dx6.shape\n",
    "dx7 = dx6.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx    = [200,400,600,800,1200]\n",
    "yy1_1 = [abs(i) for i in list(dx8[1::2])]\n",
    "yy1_2 = [abs(i) for i in list(dx8[2::2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy2_1 = [abs(i) for i in list(dx8[1::2])]\n",
    "yy2_2 = [abs(i) for i in list(dx8[2::2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy3_1 = [abs(i) for i in list(dx8[1::2])]\n",
    "yy3_2 = [abs(i) for i in list(dx8[2::2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph Reflectance Band 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "for i,j in zip([yy1_1, yy2_1, yy3_1], ['2018-05-01','2018-05-02','2018-05-03']):# , [yy1_2, yy2_2, yy3_2]):\n",
    "    plt.plot(xx,i, label=j)\n",
    "plt.legend()\n",
    "plt.xlabel('Subsamples')\n",
    "plt.ylabel('Percentage Difference')\n",
    "plt.title('Reflectance_Band_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph Reflectance Band 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "for i,j in zip([yy1_2, yy2_2, yy3_2], ['2018-05-01','2018-05-02','2018-05-03']):# , [yy1_2, yy2_2, yy3_2]):\n",
    "    plt.plot(xx,i, label=j)\n",
    "plt.legend()\n",
    "plt.xlabel('Subsamples')\n",
    "plt.ylabel('Percentage Difference')\n",
    "plt.title('Reflectance_Band_2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
