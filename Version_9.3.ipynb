{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #---------------------------------------------------- To read the files or folders in a system directory\n",
    "import re #------------------------------------------------------ To replace characters in a string\n",
    "import numpy    as np\n",
    "import datetime\n",
    "import warnings #------------------------------------------------ To suppress warnings\n",
    "import h5py #---------------------------------------------------- To read hdf5 files\n",
    "import pandas   as pd\n",
    "import time as TTT\n",
    "from   scipy                         import spatial #-------------------------------------- To extract the values and indices of k nearest neighbors\n",
    "from   ast                           import literal_eval #----------------------------------- For literal evaluation of a string to extract python objects\n",
    "from   pyproj                        import Proj, transform #----------------------------- To interconvert different projections\n",
    "from   netCDF4                       import Dataset #------------------------------------ To read nc , nc4 and hdf4 files\n",
    "from   photutils.utils               import ShepardIDWInterpolator as idw #------ To use Shepard's Inverse Distance Weighing Interpolation tool\n",
    "from   IPython.core.interactiveshell import InteractiveShell  #---- Output all jupyter lab inputs instead of the last one\n",
    "from   IPython.display               import Markdown, display\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.simplefilter('ignore')\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "### Define Non-Iterative Functions and Variables\n",
    "\n",
    "in_proj  = Proj('+proj=sinu +R=6371007.181 +nadgrids=@null +wktext') #------ Specify input projection\n",
    "out_proj = Proj(init='epsg:4326') #----------------------------------------- Specify output projection\n",
    "\n",
    "\n",
    "def Times(x): #------------------------------------------Extract Time from sounding ID. NOTE the time format is HH:MM:SSSS\n",
    "    y   = str(x)\n",
    "    yy  = y[8:]\n",
    "    yyy = '{}:{}:{}'.format(yy[:2], yy[2:4], yy[4:])\n",
    "    return yyy\n",
    "\n",
    "hours = [0, 3000000, 6000000, 9000000, 12000000, 15000000, 18000000, 21000000, 23595900]\n",
    "def f(x): #---------------------------------------------Extract the hour interval of sif time (which  is in seconds)\n",
    "    for i in range(len(hours)):\n",
    "        if (x>hours[i]) and (x<hours[i+1]):\n",
    "            lb = hours[i]\n",
    "            ub = hours[i+1]\n",
    "            return lb,ub\n",
    "            break\n",
    "\n",
    "def format_time(t): #----------------------------------- Format the time into HH:MM:SSSS for the raw format HH:MM:SSSSSSSS\n",
    "    s = t\n",
    "    return s[:-4]\n",
    "\n",
    "def nn(latitude_list,longitude_list,target): #---------- Find the index of nearest neighbor (NOTE: absolute difference)\n",
    "    target_lat, target_lon = target[1], target[0]\n",
    "    d = [abs(latitude-target_lat) + abs(longitude-target_lon) for latitude,longitude in zip(latitude_list,longitude_list)]\n",
    "    return np.argmin(d)\n",
    "\n",
    "data = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3)\n",
    "def tile_finder(Lat,Lon): #----------------------------- Find modis tile numbers in which the argument lat,lon lies\n",
    "    in_tile = False\n",
    "    i = 0\n",
    "    while(not in_tile):\n",
    "        in_tile = Lat >= data[i, 4] and Lat <= data[i, 5] and Lon >= data[i, 2] and Lon <= data[i, 3]\n",
    "        i += 1\n",
    "    V = str(int(data[i-1, 0])).zfill(2)\n",
    "    H = str(int(data[i-1, 1])).zfill(2)\n",
    "    return H,V\n",
    "\n",
    "def extract_pixel_coordinates(ULx,Uly,LRx,LRy,shape):\n",
    "    x        = np.linspace(ULx, LRx, shape[0], endpoint=False) + abs((ULx-LRx)/(2*shape[0]))\n",
    "    y        = np.linspace(ULy, LRy, shape[0], endpoint=False) - abs((ULy-LRy)/(2*shape[0]))\n",
    "    xx, yy   = np.meshgrid(x,y)\n",
    "    xs       = xx.flatten()\n",
    "    ys       = yy.flatten()\n",
    "    plon, plat = transform(in_proj, out_proj, xs, ys)\n",
    "    return plon, plat\n",
    "\n",
    "def temporal_interpolation(time1,val1,time2,val2,timeX):\n",
    "    df    = pd.DataFrame( [(time1, val1) , (time2, val2)] , columns=['Times','Values'] ) \n",
    "    df    = df.set_index('Times')\n",
    "    df    = pd.Series(df['Values'], index=df.index)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    inter = df.resample('S').interpolate(method='linear')\n",
    "    valX  = inter.loc[timeX]\n",
    "    return valX\n",
    "\n",
    "fpar_folder_list  = glob.glob('MCD15A3H/*')    \n",
    "par_folder_list   = glob.glob('MCD18A2/*' )\n",
    "ref_folder_list   = glob.glob('MCD43A4/*' )\n",
    "\n",
    "sif_file_list     = glob.glob('OCO2_sif/*.nc4')    #--------------------------------------------- List of all OCO2 files\n",
    "calipso_file_list = glob.glob('OCO2_calipso/*.h5') #--------------------------------------------- List of all OCO2-CALIPSO files\n",
    "\n",
    "data              = np.genfromtxt('sn_bound_10deg.txt', skip_header = 7, skip_footer = 3) #------ File having tile numbers and IDs\n",
    "\n",
    "for sif_file in sif_file_list: #---------------------------------------------------------------------------Read one sif file at a time\n",
    "    TTT1 = TTT.time()\n",
    "    \n",
    "    sif_date        = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%Y-%m-%d\") #-----Extract sif date\n",
    "    sif_julian_day  = datetime.datetime.strptime(sif_file.split('_')[3], '%y%m%d').strftime(\"%j\") #-----------Extract sif julian day\n",
    "    sif             = Dataset(sif_file, mode='r') #-----------------------------------------------------------Open sif file\n",
    "    calipso_df_list = [] #------------------------------------------------------------------------------------Create an empty list\n",
    "    for calipso_file in calipso_file_list: #------------------------------------------------------------------Loop through all calipso files\n",
    "        calipso_date      = datetime.datetime.strptime(calipso_file.split('_')[5], '%y%m%d').strftime(\"%Y-%m-%d\")\n",
    "        if calipso_date  == sif_date: #----------------------------------------------------------------------------If calipso date matches sif date,\n",
    "            calipso       = h5py.File(calipso_file, mode='r') #----------------------------------------------------open the calipso file\n",
    "            calipso_ID    = calipso['OCO2_sounding_id'                                           ][:]\n",
    "            calipso_dist  = calipso['matchup_distance_km'                                        ][:]\n",
    "            calipso_index = calipso['matchup_Xindex'                                             ][:]\n",
    "            calipso_dfs   = pd.DataFrame({'sounding_id':calipso_ID.flatten(),'Xindex':calipso_index.flatten(),'Xdistance':calipso_dist.flatten()}) #-----Create dataframe with variables\n",
    "            calipso_dfs[calipso_dfs.Xindex==-999.0] = np.nan #------- Replace missing values with nan\n",
    "            calipso_dfs.dropna(inplace=True) #----------------------- Drop missing values of Xindex\n",
    "            calipso_dfs[calipso_dfs.Xdistance>=2.0] = np.nan\n",
    "            calipso_dfs.dropna(inplace=True)\n",
    "            calipso_df_list.append(calipso_dfs) #-------------------- Add all calipso dataframes into a list\n",
    "    calipso_df                     = pd.concat(calipso_df_list, ignore_index = True).drop_duplicates() #---------Create a final calipso dataframe for a day\n",
    "\n",
    "    cloud_albedo                   = sif.groups['Cloud'].variables['albedo'                 ][:].flatten() #--------Read sif variables and flatten them\n",
    "    cloud_flag                     = sif.groups['Cloud'].variables['cloud_flag'             ][:].flatten()\n",
    "    cloud_co2_ratio                = sif.groups['Cloud'].variables['co2_ratio'              ][:].flatten()\n",
    "    cloud_delta_surface_pressure   = sif.groups['Cloud'].variables['delta_surface_pressure' ][:].flatten()\n",
    "    cloud_o2_ratio                 = sif.groups['Cloud'].variables['o2_ratio'               ][:].flatten()\n",
    "    vapor_pressure_deficit         = sif.groups['Meteo'].variables['vapor_pressure_deficit' ][:].flatten()\n",
    "    temperature_2m                 = sif.groups['Meteo'].variables['2m_temperature'         ][:].flatten()\n",
    "    temperature_skin               = sif.groups['Meteo'].variables['skin_temperature'       ][:].flatten()\n",
    "    specific_humidity              = sif.groups['Meteo'].variables['specific_humidity'      ][:].flatten()\n",
    "    surface_pressure               = sif.groups['Meteo'].variables['surface_pressure'       ][:].flatten()\n",
    "    wind_speed                     = sif.groups['Meteo'].variables['wind_speed'             ][:].flatten()\n",
    "    continuum_radiance_757nm       = sif.variables         ['continuum_radiance_757nm'      ][:].flatten()\n",
    "    continuum_radiance_771nm       = sif.variables         ['continuum_radiance_771nm'      ][:].flatten()\n",
    "    daily_correction_factor        = sif.variables         ['daily_correction_factor'       ][:].flatten()\n",
    "    footprint                      = sif.variables         ['footprint'                     ][:].flatten()\n",
    "    IGBP_index                     = sif.variables         ['IGBP_index'                    ][:].flatten()\n",
    "    latitude                       = sif.variables         ['latitude'                      ][:].flatten()\n",
    "    longitude                      = sif.variables         ['longitude'                     ][:].flatten()\n",
    "    measurement_mode               = sif.variables         ['measurement_mode'              ][:].flatten()\n",
    "    orbit_number                   = sif.variables         ['orbit_number'                  ][:].flatten()\n",
    "    reduced_chi2_757nm             = sif.variables         ['reduced_chi2_757nm'            ][:].flatten()\n",
    "    reduced_chi2_771nm             = sif.variables         ['reduced_chi2_771nm'            ][:].flatten()\n",
    "    sensor_azimuth_angle           = sif.variables         ['sensor_azimuth_angle'          ][:].flatten()\n",
    "    sensor_zenith_angle            = sif.variables         ['sensor_zenith_angle'           ][:].flatten()\n",
    "    SIF_757nm                      = sif.variables         ['SIF_757nm'                     ][:].flatten()\n",
    "    SIF_757nm_relative             = sif.variables         ['SIF_757nm_relative'            ][:].flatten()\n",
    "    SIF_757nm_uncert               = sif.variables         ['SIF_757nm_uncert'              ][:].flatten()\n",
    "    SIF_771nm                      = sif.variables         ['SIF_771nm'                     ][:].flatten()\n",
    "    SIF_771nm_relative             = sif.variables         ['SIF_771nm_relative'            ][:].flatten()\n",
    "    SIF_771nm_uncert               = sif.variables         ['SIF_771nm_uncert'              ][:].flatten()\n",
    "    solar_azimuth_angle            = sif.variables         ['solar_azimuth_angle'           ][:].flatten()\n",
    "    solar_zenith_angle             = sif.variables         ['solar_zenith_angle'            ][:].flatten()\n",
    "    sounding_id                    = sif.variables         ['sounding_id'                   ][:].flatten()\n",
    "    surface_altitude               = sif.variables         ['surface_altitude'              ][:].flatten()\n",
    "    time                           = sif.variables         ['time'                          ][:].flatten()\n",
    "    uncorrected_SIF_757nm          = sif.variables         ['uncorrected_SIF_757nm'         ][:].flatten()\n",
    "    uncorrected_SIF_757nm_relative = sif.variables         ['uncorrected_SIF_757nm_relative'][:].flatten()\n",
    "    uncorrected_SIF_771nm          = sif.variables         ['uncorrected_SIF_771nm'         ][:].flatten()\n",
    "    uncorrected_SIF_771nm_relative = sif.variables         ['uncorrected_SIF_771nm_relative'][:].flatten()\n",
    "\n",
    "      \n",
    "    sif_rows  = [(SIF_757nm[i], cloud_albedo[i], cloud_flag[i], cloud_co2_ratio[i], cloud_delta_surface_pressure[i], cloud_o2_ratio[i], vapor_pressure_deficit[i],\n",
    "                  temperature_2m[i], temperature_skin[i], specific_humidity[i], surface_pressure[i], wind_speed[i], continuum_radiance_757nm[i],\n",
    "                  continuum_radiance_771nm[i],daily_correction_factor[i], footprint[i], IGBP_index[i], latitude[i], longitude[i], measurement_mode[i],\n",
    "                  orbit_number[i], reduced_chi2_757nm[i],reduced_chi2_771nm[i], sensor_azimuth_angle[i], sensor_zenith_angle[i], SIF_757nm_relative[i],\n",
    "                  SIF_757nm_uncert[i], SIF_771nm[i],SIF_771nm_relative[i], SIF_771nm_uncert[i], solar_azimuth_angle[i], solar_zenith_angle[i], sounding_id[i],\n",
    "                  surface_altitude[i], time[i], uncorrected_SIF_757nm[i], uncorrected_SIF_757nm_relative[i], uncorrected_SIF_771nm[i], uncorrected_SIF_771nm_relative[i])\n",
    "                  for i in range(0,len(sounding_id))]\n",
    "    \n",
    "    column_labels = ['SIF_757nm', 'cloud_albedo', 'cloud_flag', 'cloud_co2_ratio', 'cloud_delta_surface_pressure', 'cloud_o2_ratio', 'vapor_pressure_deficit',\n",
    "                     'temperature_2m', 'temperature_skin', 'specific_humidity', 'surface_pressure', 'wind_speed', 'continuum_radiance_757nm',\n",
    "                     'continuum_radiance_771nm','daily_correction_factor', 'footprint', 'IGBP_index', 'latitude', 'longitude', 'measurement_mode',\n",
    "                     'orbit_number', 'reduced_chi2_757nm','reduced_chi2_771nm', 'sensor_azimuth_angle', 'sensor_zenith_angle',\n",
    "                     'SIF_757nm_relative', 'SIF_757nm_uncert', 'SIF_771nm','SIF_771nm_relative', 'SIF_771nm_uncert', 'solar_azimuth_angle', 'solar_zenith_angle',\n",
    "                     'sounding_id', 'surface_altitude', 'time','uncorrected_SIF_757nm', 'uncorrected_SIF_757nm_relative', 'uncorrected_SIF_771nm',\n",
    "                     'uncorrected_SIF_771nm_relative']\n",
    "     \n",
    "    sif_df                         = pd.DataFrame(sif_rows,columns = column_labels) #-------- Create sif variables' dataframe\n",
    "    \n",
    "    \n",
    "    calipso_sif_merger             = pd.merge(sif_df, calipso_df, on = ['sounding_id'], how = 'inner') #--------Merge sif and calipso on sounding _id\n",
    "\n",
    "    calipso_sif_merger['Date']     = calipso_sif_merger['sounding_id'].map(lambda x: '-'.join([str(x)[:4],str(x)[4:6],str(x)[6:]])[:10]) #-----Create new date column\n",
    "    calipso_sif_merger['SIF_Time'] = calipso_sif_merger['sounding_id'].map(lambda x: Times(x))  #----------------------------------------------Create new time column\n",
    "    calipso_sif_merger['tile_h'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[0], axis=1) #------Create new horizontal tile column\n",
    "    calipso_sif_merger['tile_v'  ] = calipso_sif_merger.apply(lambda x: tile_finder(x['latitude'], x['longitude'])[1], axis=1) #------Create new vertical tile column\n",
    "    calipso_sif_merger             = calipso_sif_merger.dropna(how='any')\n",
    "    calipso_sif_merger.to_csv('Processed_sif/df_sif_{}.csv'.format(sif_date), index=False)\n",
    "    grp         = calipso_sif_merger.groupby(['tile_h', 'tile_v']).agg(lambda x: list(x))  #----Group sif-calipso merger(from now on called SIF*) by tile id\n",
    "    grp         = grp.reset_index() #-----------------------------------------------------------Reset indices\n",
    "    l_ungrouped = len(calipso_sif_merger)\n",
    "    l_grouped   = len(grp)\n",
    "    \n",
    "    df          = grp.copy() #--------make a copy of the grouped file\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    printmd('**For {}, there are {} sif footprints scattered over {} tiles.**'.format(sif_date, l_ungrouped, l_grouped))\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------')  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # START PAR PROCESSING\n",
    "    \n",
    "\n",
    "    print('PAR Processing Started...\\n')\n",
    "    shape              = (240,240)\n",
    "    Each_Par_Tile_Data = []   #-------------Create empty list to store all the par extracted data as tuples\n",
    "    \n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'],df['sounding_id']): #Loop through rows\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')\n",
    "        \n",
    "        for folder_number in range(len(par_folder_list)): #-------------------------Go inside folders of daily par files\n",
    "            par_julian_day    = par_folder_list[folder_number].split('/')[1] #------Extract par julian day\n",
    "            \n",
    "            if sif_julian_day == par_julian_day: #------------------------------------If sif julian day is same as par folder julian day,\n",
    "                par_file_list = glob.glob(par_folder_list[folder_number]+'/*.hdf')#---open that folder (this folder contains par files for that day)\n",
    "            \n",
    "                for par_file in par_file_list: #--------------------------------------Loop through par files in par file list created in above line\n",
    "                    h_par = par_file.split('.')[2][1:3] #---------------Extract par h tile no.\n",
    "                    v_par = par_file.split('.')[2][4:6] #---------------Extract par v tile no.\n",
    "                    \n",
    "                    if (h_par==h_sif) and (v_par==v_sif): #-------------If sif tiles match with par tiles, open par file otherwise go to the next par file to check.\n",
    "                        par       = Dataset(par_file, mode='r')\n",
    "                        par_date  = datetime.datetime.strptime(par_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                        gmt_0000  = par.variables['GMT_0000_PAR'][:].flatten() #-----------------------------------------Read par 3-hourly variables\n",
    "                        gmt_0300  = par.variables['GMT_0300_PAR'][:].flatten()\n",
    "                        gmt_0600  = par.variables['GMT_0600_PAR'][:].flatten()\n",
    "                        gmt_0900  = par.variables['GMT_0900_PAR'][:].flatten()\n",
    "                        gmt_1200  = par.variables['GMT_1200_PAR'][:].flatten()\n",
    "                        gmt_1500  = par.variables['GMT_1500_PAR'][:].flatten()\n",
    "                        gmt_1800  = par.variables['GMT_1800_PAR'][:].flatten()\n",
    "                        gmt_2100  = par.variables['GMT_2100_PAR'][:].flatten()\n",
    "                        struct    = getattr(par, 'StructMetadata.0')\n",
    "                        struct1   = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3] #-------------]\n",
    "                        struct2   = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3] #-------------] Extract upper right and lower left\n",
    "                        ULx, ULy  = literal_eval(struct1) #-----------------------------------------------------------------------] coordinates of the tile of opened par file\n",
    "                        LRx, LRy  = literal_eval(struct2) #-----------------------------------------------------------------------]\n",
    "                        par_lon,par_lat = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape) #----------- Extract the par lat,lon meshgrid in proper projection \n",
    "                        tree      = spatial.KDTree(   list(  zip(par_lon, par_lat) )) #---------------- Create a nearest neighbor spatial tree\n",
    "\n",
    "                        for sub in range(len(sif_time)): #--------------------------Loop through the list of a dataframe cell (grouped dataframe cells consist of lists of values)\n",
    "                            target          = (sif_lat[sub] , sif_lon[sub])  #---------------------------------------------------Make sif coordinates target for spatial interpolation\n",
    "                            sif_time_sub    = re.sub(':', '', sif_time[sub]) #---------------------------------------------------Remove : from sif_time\n",
    "                            timeX           = pd.to_datetime(sif_time_sub.ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")#--Change resolution from SSSS to SS for faster interpolation\n",
    "                            lower_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[0]).zfill(8)[:4] #---Find lower bound of interval in which sif time lies\n",
    "                       \n",
    "                            if f(int(sif_time_sub))[1] == 23595900: #-----------------------------------------------If upper bound is greater than 2100 hours that is 235959\n",
    "                                upper_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[1]).zfill(8)[:6] #----upper key becomes interp_gmt_235959\n",
    "                            else:                                                                              #----else                   \n",
    "                                upper_bound_key = str('interp_gmt_')+str(f(int(sif_time_sub))[1]).zfill(8)[:4] #----upper key is as it is (interp_gmt_1200,1500,1800,etc)\n",
    "                        \n",
    "                            neigh5          = tree.query([(sif_lon[sub], sif_lat[sub])], k=10)[1][0] #---Find k=3 nearest spatial neighbors indices at target\n",
    "                            lon_for_idw     = [par_lon[i] for i in neigh5] #----------------------------Extract longitudes at these indices\n",
    "                            lat_for_idw     = [par_lat[i] for i in neigh5] #----------------------------Extract latitudes also\n",
    "                            coors_for_idw   = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] #---------Create a list of coordinates (lat lon tuples)\n",
    "\n",
    "                            gmt_0000n       = [gmt_0000[i] for i in neigh5] #---------------------------Find variable values at nearest neighbors\n",
    "                            gmt_0300n       = [gmt_0300[i] for i in neigh5]\n",
    "                            gmt_0600n       = [gmt_0600[i] for i in neigh5]\n",
    "                            gmt_0900n       = [gmt_0900[i] for i in neigh5]\n",
    "                            gmt_1200n       = [gmt_1200[i] for i in neigh5]\n",
    "                            gmt_1500n       = [gmt_1500[i] for i in neigh5]\n",
    "                            gmt_1800n       = [gmt_1800[i] for i in neigh5]\n",
    "                            gmt_2100n       = [gmt_2100[i] for i in neigh5]\n",
    "\n",
    "                            func_gmt_0000   = idw(coors_for_idw, gmt_0000n) #---------------------------Create inverse distance weighing (IDW) interpolation function at nn coordinates\n",
    "                            func_gmt_0300   = idw(coors_for_idw, gmt_0300n)\n",
    "                            func_gmt_0600   = idw(coors_for_idw, gmt_0600n)\n",
    "                            func_gmt_0900   = idw(coors_for_idw, gmt_0900n)\n",
    "                            func_gmt_1200   = idw(coors_for_idw, gmt_1200n)\n",
    "                            func_gmt_1500   = idw(coors_for_idw, gmt_1500n)\n",
    "                            func_gmt_1800   = idw(coors_for_idw, gmt_1800n)\n",
    "                            func_gmt_2100   = idw(coors_for_idw, gmt_2100n)\n",
    "\n",
    "                            interp_gmt_0000 = func_gmt_0000(target) #-----------------------------------Find interpolated hourly par at target\n",
    "                            interp_gmt_0300 = func_gmt_0300(target)\n",
    "                            interp_gmt_0600 = func_gmt_0600(target)\n",
    "                            interp_gmt_0900 = func_gmt_0900(target)\n",
    "                            interp_gmt_1200 = func_gmt_1200(target)\n",
    "                            interp_gmt_1500 = func_gmt_1500(target)\n",
    "                            interp_gmt_1800 = func_gmt_1800(target)\n",
    "                            interp_gmt_2100 = func_gmt_2100(target)\n",
    "\n",
    "                            interp_gmt_235959 = 0\n",
    "                            mydict = {'interp_gmt_0000':interp_gmt_0000, 'interp_gmt_0300':interp_gmt_0300, 'interp_gmt_0600'  :interp_gmt_0600,\n",
    "                                      'interp_gmt_0900':interp_gmt_0900, 'interp_gmt_1200':interp_gmt_1200, 'interp_gmt_1500'  :interp_gmt_1500,\n",
    "                                      'interp_gmt_1800':interp_gmt_1800, 'interp_gmt_2100':interp_gmt_2100, 'interp_gmt_235959':interp_gmt_235959} #---Save variables in dictionary \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                            lower_bound_value = mydict[lower_bound_key] #-----Find lower 3 hourly par value\n",
    "                            upper_bound_value = mydict[upper_bound_key] #-----Find upper 3 hourly par value\n",
    "\n",
    "                            if upper_bound_value == interp_gmt_235959:  #---------------------------- If upper par value > 2100, we need to extract its value from next day par file\n",
    "                                par_file_list = glob.glob(par_folder_list[folder_number+1]+'/*.hdf') #---Open next day par file list\n",
    "\n",
    "                                for par_file in par_file_list:  #----------------------------]\n",
    "                                    h_par = par_file.split('.')[2][1:3]#---------------------] Check for date and tile\n",
    "                                    v_par = par_file.split('.')[2][4:6]#---------------------]\n",
    "\n",
    "                                    if (h_par == h_sif) and (v_par == v_sif): #-----------------------------------------] Repeat above procedure\n",
    "                                        par = Dataset(par_file, mode='r') #---------------------------------------------] once again just\n",
    "                                        gmt_235959        = par.variables['GMT_0000_PAR'][:].flatten() #----------------] to obtain the spatially\n",
    "                                        lon_for_idw       = [par_lon[i] for i in neigh5] #------------------------------] interpolated par \n",
    "                                        lat_for_idw       = [par_lat[i] for i in neigh5] #------------------------------] value at the upper bound\n",
    "                                        coors_for_idw     = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] #-----------]\n",
    "                                        gmt_235959        = [gmt_235959[i] for i in neigh5] #---------------------------]\n",
    "                                        func_gmt_235959   = idw(coors_for_idw, gmt_235959) #----------------------------]\n",
    "                                        interp_gmt_235959 = func_gmt_235959(target) #-----------------------------------]\n",
    "                                        break\n",
    "                                upper_bound_value = interp_gmt_235959\n",
    "                            \n",
    "                            # Do TIME INTERPOLATION\n",
    "                            time1a = pd.to_datetime(lower_bound_key[11:].ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "                            time2a = pd.to_datetime(upper_bound_key[11:].ljust(8, \"0\"), format=\"%H%M%S%f\").strftime(\"%H:%M:%S\")\n",
    "                            val1   = lower_bound_value\n",
    "                            val2   = upper_bound_value\n",
    "                            valX   = temporal_interpolation(time1a,val1,time2a,val2,timeX)\n",
    "                            Each_Par_Tile_Data.append((sif_sid[sub],sif_lat[sub],sif_lon[sub],valX)) #--------Add sif sounding id, sif lat, sif lon and final interpolated value to list\n",
    "    \n",
    "    DF = pd.DataFrame(np.array(Each_Par_Tile_Data), columns=['sounding_id','latitude','longitude', 'par']) #---------------Write each list of par data in a dataframe\n",
    "    DF.to_csv('Processed_par/df_par_{}.csv'.format(sif_date),index=False)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    # START REF PROCESSING\n",
    "    \n",
    "    print('REF Processing Started...\\n')\n",
    "    shape2              = (2400,2400)\n",
    "    Each_Ref_Tile_Data  = []\n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'],df['sounding_id']):\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ')\n",
    "        \n",
    "        for folder_number in range(len(ref_folder_list)):\n",
    "            ref_julian_day    = ref_folder_list[folder_number].split('/')[1]\n",
    "            \n",
    "            if sif_julian_day == ref_julian_day:\n",
    "                ref_file_list = glob.glob(ref_folder_list[folder_number]+'/*.hdf')\n",
    "            \n",
    "                for num3,ref_file in enumerate(ref_file_list):\n",
    "                    h_ref = ref_file.split('.')[2][1:3]\n",
    "                    v_ref = ref_file.split('.')[2][4:6]\n",
    "                    \n",
    "                    if (h_ref==h_sif) and (v_ref==v_sif):\n",
    "                        ref_date          = datetime.datetime.strptime(ref_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                        ref               = Dataset(ref_file, mode='r')\n",
    "                        struct            = getattr(ref, 'StructMetadata.0')\n",
    "                        struct1           = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3]\n",
    "                        struct2           = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3]\n",
    "                        ULx, ULy          = literal_eval(struct1)\n",
    "                        LRx, LRy          = literal_eval(struct2)\n",
    "                        nrb1x             = ref.variables['Nadir_Reflectance_Band1'][:].flatten() \n",
    "                        nrb2x             = ref.variables['Nadir_Reflectance_Band2'][:].flatten()\n",
    "                        ref_lonx,ref_latx = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape2)\n",
    "                        ref_lat           = ref_latx[(nrb1x.mask == False) & (nrb2x.mask == False)] #----- Drop missing values\n",
    "                        ref_lon           = ref_lonx[(nrb1x.mask == False) & (nrb2x.mask == False)] #----- values where\n",
    "                        nrb1              = nrb1x   [(nrb1x.mask == False) & (nrb2x.mask == False)] #----- mask == True\n",
    "                        nrb2              = nrb2x   [(nrb1x.mask == False) & (nrb2x.mask == False)] #----- for both reflectance bands\n",
    "                        tree              = spatial.KDTree( list(  zip(ref_lon, ref_lat) ))\n",
    "                    \n",
    "                        for sub in range(len(sif_time)):\n",
    "                            target         = (sif_lat[sub] , sif_lon[sub])\n",
    "                            neigh          = tree.query([(sif_lon[sub], sif_lat[sub])], k=4)[1][0] #---Find k=4 nearest spatial neighbors indices at target\n",
    "                            lon_for_idw    = [ref_lon[i] for i in neigh] #----------------------------Extract longitudes at these indices\n",
    "                            lat_for_idw    = [ref_lat[i] for i in neigh] #----------------------------Extract latitudes also\n",
    "                            coors_for_idw  = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] \n",
    "                            nrb_01         = [nrb1[i] for i in neigh]  #---------------------------Find variable values at nearest neighbors\n",
    "                            nrb_02         = [nrb2[i] for i in neigh]\n",
    "                            func_nrb_01    = idw(coors_for_idw, nrb_01) #---------------------------Create inverse distance weighing (IDW) interpolation function at nn coordinates\n",
    "                            func_nrb_02    = idw(coors_for_idw, nrb_02)\n",
    "                            interp_nrb1    = func_nrb_01(target) #-----------------------------------Find interpolated hourly par at target\n",
    "                            interp_nrb2    = func_nrb_02(target)\n",
    "\n",
    "                            Each_Ref_Tile_Data.append((sif_sid[sub], sif_lat[sub], sif_lon[sub], interp_nrb1, interp_nrb2))\n",
    "                \n",
    "                        \n",
    "    DG = pd.DataFrame(np.array(Each_Ref_Tile_Data),columns=['sounding_id','latitude','longitude','nrb1','nrb2'])\n",
    "    DG.to_csv('Processed_ref/df_ref_{}.csv'.format(sif_date),index=False)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    # START FPAR PROCESSING    \n",
    "\n",
    "    print('FPAR Processing Started...\\n')\n",
    "    shape3               = (2400,2400)\n",
    "    Each_fpar_Tile_Data  = []\n",
    "    for index,h_sif,v_sif,sif_lon,sif_lat,sif_time,sif_sid in zip(df.index,df['tile_h'],df['tile_v'],df['longitude'],df['latitude'],df['SIF_Time'], df['sounding_id']):\n",
    "        print('h{}v{}'.format(h_sif,v_sif), flush = True, sep=',', end=' ') \n",
    "        \n",
    "        for folder_number in range(len(fpar_folder_list)):\n",
    "            fpar_julian_day    = fpar_folder_list[folder_number].split('/')[1]\n",
    "\n",
    "            if int(fpar_julian_day) in [int(sif_julian_day), int(sif_julian_day)+1, int(sif_julian_day)+2, int(sif_julian_day)+3]:\n",
    "                fpar_file_list = glob.glob(fpar_folder_list[folder_number]+'/*.hdf')\n",
    "            \n",
    "                for num3,fpar_file in enumerate(fpar_file_list):\n",
    "                    h_fpar = fpar_file.split('.')[2][1:3]\n",
    "                    v_fpar = fpar_file.split('.')[2][4:6]\n",
    "\n",
    "                    if (h_fpar==h_sif) and (v_fpar==v_sif):\n",
    "                        fpar_date           = datetime.datetime.strptime(fpar_file.split('.')[1][1:], '%Y%j').strftime(\"%Y-%m-%d\")\n",
    "                        fpar                = Dataset(fpar_file, mode='r')             \n",
    "                        struct              = getattr(fpar, 'StructMetadata.0')\n",
    "                        struct1             = struct[struct.find('UpperLeftPointMtrs'): struct.find('LowerRightMtrs')][19:-3]\n",
    "                        struct2             = struct[struct.find('LowerRightMtrs')    : struct.find('Projection')    ][15:-3]\n",
    "                        ULx, ULy            = literal_eval(struct1)\n",
    "                        LRx, LRy            = literal_eval(struct2)\n",
    "                        fpar500x            = fpar.variables['Fpar_500m'][:].flatten()\n",
    "                        lai500x             = fpar.variables['Lai_500m' ][:].flatten()\n",
    "                        fpar_lonx,fpar_latx = extract_pixel_coordinates(ULx,ULy,LRx,LRy,shape3)\n",
    "                        fpar_lon            = fpar_lonx[(fpar500x.mask == False) & (lai500x.mask == False)]\n",
    "                        fpar_lat            = fpar_latx[(fpar500x.mask == False) & (lai500x.mask == False)]\n",
    "                        fpar500             = fpar500x [(fpar500x.mask == False) & (lai500x.mask == False)]\n",
    "                        lai500              = lai500x  [(fpar500x.mask == False) & (lai500x.mask == False)]                \n",
    "                        tree                = spatial.KDTree( list(  zip(fpar_lon, fpar_lat) ))\n",
    "\n",
    "                        for sub in range(len(sif_time)):\n",
    "                            target          = (sif_lat[sub] , sif_lon[sub])\n",
    "                            neigh           = tree.query([(sif_lon[sub], sif_lat[sub])], k=4)[1][0] #---Find k=4 nearest spatial neighbors indices at target\n",
    "                            lon_for_idw     = [fpar_lon[i] for i in neigh] #----------------------------Extract longitudes at these indices\n",
    "                            lat_for_idw     = [fpar_lat[i] for i in neigh] #----------------------------Extract latitudes also\n",
    "                            coors_for_idw   = [(i,j) for i,j in zip(lat_for_idw,lon_for_idw)] \n",
    "                            fpar500_01      = [fpar500[i] for i in neigh]  #---------------------------Find variable values at nearest neighbors\n",
    "                            lai500_02       = [lai500 [i] for i in neigh]\n",
    "                            func_fpar500_01 = idw(coors_for_idw, fpar500_01) #---------------------------Create inverse distance weighing (IDW) interpolation function at nn coordinates\n",
    "                            func_lai500_02  = idw(coors_for_idw, lai500_02)\n",
    "                            interp_fpar500  = func_fpar500_01(target) #-----------------------------------Find interpolated hourly par at target\n",
    "                            interp_lai500   = func_lai500_02(target)\n",
    "                            #ind        = tree.query([(sif_lon[sub],sif_lat[sub])], k=1)[1][0] #-----------In case nearest value needs to be taken as it is\n",
    "                            Each_fpar_Tile_Data.append((sif_sid[sub],sif_lat[sub],sif_lon[sub],interp_fpar500, interp_lai500))\n",
    "\n",
    "                        \n",
    "    DH = pd.DataFrame(np.array(Each_fpar_Tile_Data),columns=['sounding_id','latitude','longitude','Fpar_500m','Lai_500m'])\n",
    "    DH.to_csv('Processed_fpar/df_fpar_{}.csv'.format(sif_date),index=False)\n",
    "\n",
    "    \n",
    "    di  = calipso_sif_merger\n",
    "    dj  = DF\n",
    "    dk  = DG\n",
    "    dl  = DH\n",
    "\n",
    "    dM1 = pd.merge(di, dj, on=['sounding_id','latitude','longitude' ],how='inner')\n",
    "    dM2 = pd.merge(dM1, dk, on=['sounding_id','latitude','longitude'],how='inner')\n",
    "    dM3 = pd.merge(dM2, dl, on=['sounding_id','latitude','longitude'],how='inner')\n",
    "\n",
    "    dM3 = dM3.rename({'par':'PAR', 'nrb1':'Nadir_Reflectance_Band1', 'nrb2':'Nadir_Reflectance_Band2'}, axis=1)\n",
    "    L = len(dM3)\n",
    "    with Dataset('OCO2_sif/oco2_LtSIF_180501_B8100r_180703004855s.nc4','r') as src_sif,\\\n",
    "    Dataset(\"Data__%s.nc4\"%i_sif[21:-4], \"w\")                                 as output ,\\\n",
    "    Dataset('MCD18A2/121/MCD18A2.A2018121.h01v08.006.2019091180956.hdf')    as src_par,\\\n",
    "    Dataset('MCD43A4/121/MCD43A4.A2018121.h00v08.006.2018130031808.hdf')    as src_ref,\\\n",
    "    Dataset('MCD15A3H/121/MCD15A3H.A2018121.h00v08.006.2018129231051.hdf')  as src_fpar:\n",
    "\n",
    "        group_sif     = output.createGroup(\"Group_OCO2\"       )\n",
    "        group_par     = output.createGroup(\"Group_MCD18A2\"    )\n",
    "        group_ref     = output.createGroup(\"Group_MCD43A4\"    )\n",
    "        group_fpar    = output.createGroup(\"Group_MCD15A3H\"   )\n",
    "\n",
    "        output.setncatts(src_sif.__dict__)\n",
    "        output.setncatts(src_par.__dict__)\n",
    "        output.setncatts(src_ref.__dict__)\n",
    "        output.setncatts(src_fpar.__dict__)\n",
    "\n",
    "        for dfname in dM3.columns:\n",
    "\n",
    "            for name, variable in src_sif.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_OCO2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_sif[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_par.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD18A2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_par[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_ref.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD43A4/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_ref[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_fpar.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD15A3H/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_fpar[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "                    \n",
    "    TTT2 = TTT.time()\n",
    "    print(np.round((TTT2-TTT1)/60),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dM2 = dM1.merge(dk, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "#len(dM2)\n",
    "#dM3 = dM2.merge(dl, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "#len(dM3)\n",
    "#dM3 = dM3.dropna(subset=['par'])#,'nrb1','nrb2','Fpar_500m','Lai_500m'])\n",
    "#3L   = len(dM3)\n",
    "#print('Dimensions:',L)\n",
    "#3dM3 = dM3.rename({'par':'PAR', 'nrb1':'Nadir_Reflectance_Band1', 'nrb2':'Nadir_Reflectance_Band2'}, axis=1)\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "sif_list  = sorted(glob.glob('Processed_sif/*.csv' ))\n",
    "par_list  = sorted(glob.glob('Processed_par/*.csv' ))\n",
    "ref_list  = sorted(glob.glob('Processed_ref/*.csv' ))\n",
    "fpar_list = sorted(glob.glob('Processed_fpar/*.csv'))\n",
    "\n",
    "for i_sif, i_par, i_ref, i_fpar in zip(sif_list, par_list, ref_list, fpar_list):\n",
    "\n",
    "    di  = pd.read_csv(i_sif, dtype={'sounding_id':float})\n",
    "    \n",
    "    dj  = pd.read_csv(i_par)\n",
    "    dk  = pd.read_csv(i_ref)\n",
    "    dl  = pd.read_csv(i_fpar)\n",
    "    len(di),len(dj),len(dk),len(dl)\n",
    "    #di['sounding_id'].nunique(),dj['sounding_id'].nunique(),dk['sounding_id'].nunique(),dl['sounding_id'].nunique()\n",
    "    #di['latitude'].nunique(),dj['latitude'].nunique(),dk['latitude'].nunique(),dl['latitude'].nunique()\n",
    "    #di['longitude'].nunique(),dj['longitude'].nunique(),dk['longitude'].nunique(),dl['longitude'].nunique()\n",
    "    dM1 = di .merge(dj, on=['sounding_id'])#,'latitude','longitude'], how='inner')\n",
    "    kk = di[(~di['sounding_id'].isin(dM1['sounding_id']))&(~di['sounding_id'].isin(dM1['sounding_id']))]\n",
    "    kk.to_csv('missing.csv',index=False)\n",
    "    #len(dM1)\n",
    "    #3dM1.info()\n",
    "    #dM2 = dM1.merge(dk, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "    #len(dM2)\n",
    "    #dM3 = dM2.merge(dl, on=['sounding_id','latitude','longitude'], how='inner')\n",
    "    #len(dM3)\n",
    "    #dM3 = dM3.dropna(subset=['par'])#,'nrb1','nrb2','Fpar_500m','Lai_500m'])\n",
    "    #3L   = len(dM3)\n",
    "    #print('Dimensions:',L)\n",
    "    #3dM3 = dM3.rename({'par':'PAR', 'nrb1':'Nadir_Reflectance_Band1', 'nrb2':'Nadir_Reflectance_Band2'}, axis=1)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#di['sounding_id'].where(di['sounding_id'].values==dj['sounding_id'].values).notna()\n",
    "l = pd.merge(di, dj, on=['sounding_id'], how='inner')\n",
    "l.shape\n",
    "pd.DataFrame(l['sounding_id']).to_csv('vvv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.shape\n",
    "DG.shape\n",
    "DH.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with Dataset('OCO2_sif/oco2_LtSIF_180501_B8100r_180703004855s.nc4','r') as src_sif,\\\n",
    "    Dataset(\"Datas__%s.nc4\"%i_sif[21:-4], \"w\")                                 as output ,\\\n",
    "    Dataset('MCD18A2/121/MCD18A2.A2018121.h01v08.006.2019091180956.hdf')    as src_par,\\\n",
    "    Dataset('MCD43A4/121/MCD43A4.A2018121.h00v08.006.2018130031808.hdf')    as src_ref,\\\n",
    "    Dataset('MCD15A3H/121/MCD15A3H.A2018121.h00v08.006.2018129231051.hdf')  as src_fpar:\n",
    "        \n",
    "        group_sif     = output.createGroup(\"Group_OCO2\"       )\n",
    "        group_par     = output.createGroup(\"Group_MCD18A2\"    )\n",
    "        group_ref     = output.createGroup(\"Group_MCD43A4\"    )\n",
    "        group_fpar    = output.createGroup(\"Group_MCD15A3H\"   )\n",
    "        \n",
    "        output.setncatts(src_sif.__dict__)\n",
    "        output.setncatts(src_par.__dict__)\n",
    "        output.setncatts(src_ref.__dict__)\n",
    "        output.setncatts(src_fpar.__dict__)\n",
    "        \n",
    "        for dfname in dM3.columns:\n",
    "            \n",
    "            for name, variable in src_sif.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_OCO2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_sif[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_par.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD18A2/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_par[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "                    \n",
    "            for name, variable in src_ref.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD43A4/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_ref[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])\n",
    "\n",
    "            for name, variable in src_fpar.variables.items():\n",
    "                if name == dfname:\n",
    "                    outvar         = '/Group_MCD15A3H/%s' %name\n",
    "                    output.createDimension(name, L)\n",
    "                    x                  = output.createVariable(outvar, variable.datatype, (name,))\n",
    "                    output[outvar].setncatts(src_fpar[name].__dict__)\n",
    "                    output[outvar][:]  = np.array(dM3[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
